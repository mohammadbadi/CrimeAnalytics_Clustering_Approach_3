{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadbadi/CrimeAnalytics_Clustering_Approach_3/blob/main/Code%20for%20Approach%203%20of%20CrimeAnalytics%20Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b6fe214",
      "metadata": {
        "id": "3b6fe214",
        "papermill": {
          "duration": 0.001814,
          "end_time": "2025-03-17T09:43:41.873362",
          "exception": false,
          "start_time": "2025-03-17T09:43:41.871548",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "\n",
        "### **5.0 Loading Libraries and Major Crime Indicator Dataset from TPS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "765716df",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-17T09:43:41.879094Z",
          "iopub.status.busy": "2025-03-17T09:43:41.878616Z",
          "iopub.status.idle": "2025-03-17T09:44:09.183296Z",
          "shell.execute_reply": "2025-03-17T09:44:09.182051Z"
        },
        "id": "765716df",
        "outputId": "44bb9c0f-6550-4936-9bf8-35b3a33cc95d",
        "papermill": {
          "duration": 27.310104,
          "end_time": "2025-03-17T09:44:09.185385",
          "exception": false,
          "start_time": "2025-03-17T09:43:41.875281",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div style=\"font-size: 18px; color: #333; font-weight: bold; padding: 10px;\">\n",
              "    Dataset <span style='color: blue;'>major-crime-indicators.csv</span> by <span style='color: slategray;'>Mohammad Badi</span> from Kaggle website is <span style='color: green;'>Successfully</span> loaded!\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div style=\"font-size: 18px; color: #333; font-weight: bold; padding: 10px;\">\n",
              "    Dataset saved in <span style='color: blue;'>current workspace</span> <span style='color: green;'>Successfully!</span>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div style=\"font-size: 18px; color: #333; font-weight: bold; padding: 10px;\">\n",
              "    Data saved as CSV: <span style='color: blue;'>Checking_Load_Time.csv</span>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div style=\"font-size: 18px; color: #333; font-weight: bold; padding: 10px;\">\n",
              "    Data saved as Excel: <span style='color: blue;'>Checking_Load_Time.xlsx</span>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div style=\"font-size: 18px; color: #333; font-weight: bold; padding: 10px;\">\n",
              "    Time taken to read <span style='color: blue;'>Checking_Load_Time CSV file</span>: <span style='color: green;'>3.35 seconds</span>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div style=\"font-size: 18px; color: #333; font-weight: bold; padding: 10px;\">\n",
              "    Time taken to read <span style='color: blue;'>Checking_Load_Time Excel file</span>: <span style='color: red;'>241.39 seconds</span>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div style=\"font-size: 18px; color: #333; font-weight: bold; padding: 10px;\">\n",
              "    Recommendation: Load the data from <span style='color: green;'>CSV</span> as it is approximately <span style='color: green;'>72.15 times faster</span> than loading from Excel.\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div style=\"font-size: 18px; color: #333; font-weight: bold; padding: 10px;\">\n",
              "    Dataset has been analyzed, and recommendation has been provided!\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "                                                                                  # Import necessary libraries\n",
        "import itertools\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import kagglehub\n",
        "import warnings\n",
        "from IPython.display import display, HTML\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "from google.colab import files\n",
        "\n",
        "os.system('pip install openpyxl -qqq')                                            # Install openpyxl for Excel support\n",
        "os.system('pip install tabulate -qqq')                                            # Install tabulate for cleaner table output\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)                    # Ignore Deprecation Warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)                         # Ignore future warnings\n",
        "\n",
        "file_path = \"major-crime-indicators.csv\"                                          # Set the file path to the filename with extension\n",
        "\n",
        "crime_df = kagglehub.load_dataset(                                                # Load the latest version of the dataset from Kaggle\n",
        "    kagglehub.KaggleDatasetAdapter.PANDAS,\n",
        "    \"mohammadbadi/crimes-in-toronto\",                                             # Updated dataset handle\n",
        "    file_path,\n",
        ")\n",
        "\n",
        "def format_message(message):                                                      # Function to format HTML messages\n",
        "    return f\"\"\"\n",
        "<div style=\"font-size: 18px; color: #333; font-weight: bold; padding: 10px;\">\n",
        "    {message}\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "load_message = format_message(                                                    # Display HTML formatted message confirming that the dataset is loaded\n",
        "    \"Dataset <span style='color: blue;'>major-crime-indicators.csv</span> by <span style='color: slategray;'>Mohammad Badi</span> from Kaggle website is <span style='color: green;'>Successfully</span> loaded!\"\n",
        ")\n",
        "display(HTML(load_message))\n",
        "\n",
        "crime_df.to_csv(\"major-crime-indicators.csv\", index=False)                        # Save the loaded dataset as a CSV file\n",
        "\n",
        "save_message = format_message(                                                    # Display HTML formatted message confirming that the dataset is saved\n",
        "    \"Dataset saved in <span style='color: blue;'>current workspace</span> <span style='color: green;'>Successfully!</span>\"\n",
        ")\n",
        "display(HTML(save_message))\n",
        "\n",
        "major_crime_df = crime_df                                                         # Reusing the dataframe loaded earlier\n",
        "def save_data(data_df, filename_base):\n",
        "    csv_filename = f\"{filename_base}.csv\"                                         # Save as CSV\n",
        "    data_df.to_csv(csv_filename, index=False)\n",
        "    csv_msg = format_message(f\"Data saved as CSV: <span style='color: blue;'>{csv_filename}</span>\")\n",
        "    display(HTML(csv_msg))\n",
        "    excel_filename = f\"{filename_base}.xlsx\"                                      # Save as Excel\n",
        "    data_df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
        "    excel_msg = format_message(f\"Data saved as Excel: <span style='color: blue;'>{excel_filename}</span>\")\n",
        "    display(HTML(excel_msg))\n",
        "\n",
        "save_data(major_crime_df, \"Checking_Load_Time\")                                   # Save the dataset as both CSV and Excel with name 'Checking_Load_Time'\n",
        "\n",
        "def measure_read_time(file_path, file_type):                                      # Function to measure file reading time\n",
        "    start_time = time.time()\n",
        "    if file_type == \"csv\":\n",
        "        pd.read_csv(file_path)\n",
        "    elif file_type == \"excel\":\n",
        "        pd.read_excel(file_path)\n",
        "    end_time = time.time()\n",
        "    return end_time - start_time\n",
        "\n",
        "csv_time = measure_read_time('Checking_Load_Time.csv', \"csv\")                     # Measure read times\n",
        "excel_time = measure_read_time('Checking_Load_Time.xlsx', \"excel\")\n",
        "\n",
        "csv_time_color = \"green\" if csv_time < excel_time else \"red\"                      # Determine color coding for time messages\n",
        "excel_time_color = \"green\" if excel_time < csv_time else \"red\"\n",
        "\n",
        "csv_time_message = format_message(                                                # Display HTML formatted time messages\n",
        "    f\"Time taken to read <span style='color: blue;'>Checking_Load_Time CSV file</span>: <span style='color: {csv_time_color};'>{csv_time:.2f} seconds</span>\"\n",
        ")\n",
        "\n",
        "excel_time_message = format_message(\n",
        "    f\"Time taken to read <span style='color: blue;'>Checking_Load_Time Excel file</span>: <span style='color: {excel_time_color};'>{excel_time:.2f} seconds</span>\"\n",
        ")\n",
        "\n",
        "display(HTML(csv_time_message))                                                   # Display the time messages\n",
        "display(HTML(excel_time_message))\n",
        "\n",
        "if csv_time < excel_time:                                                         # Determine the recommendation based on time\n",
        "    speed_factor = excel_time / csv_time\n",
        "    recommendation = (\n",
        "        f\"Recommendation: Load the data from <span style='color: green;'>CSV</span> as it is approximately \"\n",
        "        f\"<span style='color: green;'>{speed_factor:.2f} times faster</span> than loading from Excel.\"\n",
        "    )\n",
        "else:\n",
        "    speed_factor = csv_time / excel_time\n",
        "    recommendation = (\n",
        "        f\"Recommendation: Load the data from <span style='color: green;'>Excel</span> as it is approximately \"\n",
        "        f\"<span style='color: green;'>{speed_factor:.2f} times faster</span> than loading from CSV.\"\n",
        "    )\n",
        "\n",
        "recommendation_message = format_message(recommendation)\n",
        "display(HTML(recommendation_message))\n",
        "\n",
        "completion_message = format_message(\"Dataset has been analyzed, and recommendation has been provided!\")\n",
        "display(HTML(completion_message))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.1 Learning Application Domain: Summary Table**"
      ],
      "metadata": {
        "id": "imUcbatut-QR"
      },
      "id": "imUcbatut-QR"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings                                                                   # Import necessary libraries\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import contextlib\n",
        "from google.colab import files\n",
        "from IPython.display import display, HTML\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "# Suppress warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)                    # Ignore Deprecation Warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)                    # Ignore future warnings\n",
        "\n",
        "file_path = \"major-crime-indicators.csv\"                                          # File path for the dataset\n",
        "\n",
        "with open(os.devnull, 'w') as fnull:                                              # Load the latest version using the correct dataset handle while suppressing the download output\n",
        "    with contextlib.redirect_stdout(fnull):\n",
        "        df = kagglehub.load_dataset(\n",
        "            KaggleDatasetAdapter.PANDAS,\n",
        "            \"mohammadbadi/crimes-in-toronto\",                                     # Dataset handle on Kaggle\n",
        "            file_path,\n",
        "        )\n",
        "\n",
        "def count_leading_trailing_spaces(column):                                        # Function to check leading/trailing spaces, ensuring the column is treated as a string\n",
        "    column = column.astype(str)\n",
        "    return column.str.startswith(' ').sum(), column.str.endswith(' ').sum()\n",
        "\n",
        "def prepare_summary_table(df):                                                    # Function to prepare the summary table with only Null values check and calculate % of Null\n",
        "    unique_values = df.nunique()                                                  # Count total, unique, and Null values for each column\n",
        "    total_values = df.count() + df.isnull().sum()                                 # Total includes NaN and Null\n",
        "    null_counts = df.isnull().sum()\n",
        "\n",
        "    null_percentages = (null_counts / total_values) * 100                         # Calculate the percentage of Null values compared to total values\n",
        "\n",
        "    leading_spaces, trailing_spaces = zip(*[count_leading_trailing_spaces(df[col]) for col in df.columns]) # Initialize column-based space checks\n",
        "\n",
        "    summary_table = pd.DataFrame({                                                # Create a summary table with the calculated values\n",
        "        \"Column\": df.columns,\n",
        "        \"Data Type\": df.dtypes,\n",
        "        \"Total Values\": total_values,\n",
        "        \"Unique Values\": unique_values,\n",
        "        \"Null Values\": null_counts,\n",
        "        \"Null %\": null_percentages.round(1),\n",
        "        \"Leading Spaces\": leading_spaces,\n",
        "        \"Trailing Spaces\": trailing_spaces\n",
        "    })\n",
        "\n",
        "    return summary_table, null_counts, null_percentages\n",
        "\n",
        "def generate_summary_html(summary_table):                                        # Function to generate HTML for the summary table\n",
        "    summary_table_html = \"\"\"\n",
        "    <style>\n",
        "        table {\n",
        "            border-collapse: collapse;\n",
        "            width: 100%;\n",
        "            font-family: Arial, sans-serif;\n",
        "        }\n",
        "        table th, table td {\n",
        "            border: 1px solid #ddd;\n",
        "            padding: 8px;\n",
        "            text-align: left;\n",
        "        }\n",
        "        table th {\n",
        "            background-color: #4CAF50;\n",
        "            color: white;\n",
        "            font-size: 1.1em;\n",
        "        }\n",
        "        table tr:nth-child(even) {background-color: #f2f2f2;}\n",
        "        table tr:hover {background-color: #ddd;}\n",
        "    </style>\n",
        "    <table>\n",
        "        <tr>\n",
        "            <th>Column</th>\n",
        "            <th>Data Type</th>\n",
        "            <th>Total Values</th>\n",
        "            <th>Unique Values</th>\n",
        "            <th>Null Values</th>\n",
        "            <th>Null %</th>\n",
        "            <th>Leading Spaces</th>\n",
        "            <th>Trailing Spaces</th>\n",
        "        </tr>\n",
        "    \"\"\"\n",
        "\n",
        "    for _, row in summary_table.iterrows():\n",
        "        null_color = \"red\" if row['Null Values'] > 0 else \"green\"\n",
        "        leading_color = \"red\" if row['Leading Spaces'] > 0 else \"green\"\n",
        "        trailing_color = \"red\" if row['Trailing Spaces'] > 0 else \"green\"\n",
        "\n",
        "        summary_table_html += f\"\"\"\n",
        "        <tr>\n",
        "            <td>{row['Column']}</td>\n",
        "            <td>{row['Data Type']}</td>\n",
        "            <td>{row['Total Values']}</td>\n",
        "            <td>{row['Unique Values']}</td>\n",
        "            <td style='color:{null_color};'>{row['Null Values']}</td>\n",
        "            <td>{row['Null %']}</td>\n",
        "            <td style='color:{leading_color};'>{row['Leading Spaces']}</td>\n",
        "            <td style='color:{trailing_color};'>{row['Trailing Spaces']}</td>\n",
        "        </tr>\n",
        "        \"\"\"\n",
        "    summary_table_html += \"</table>\"\n",
        "    return summary_table_html\n",
        "\n",
        "def plot_missing_percentage(df, dataset_name):                                    # Function to plot missing percentage visualization\n",
        "    missing_percentage = df.isnull().mean() * 100                                 # Calculate the missing percentage for each column\n",
        "    missing_percentage = missing_percentage[missing_percentage > 0]               # show only columns with missing values\n",
        "\n",
        "    if missing_percentage.empty:                                                  # Check if there's any column with missing data\n",
        "        print(f\"No missing data in {dataset_name}. Skipping missing percentage plot.\")\n",
        "        return \"\"\n",
        "\n",
        "    plt.figure(figsize=(10, 6))                                                  # Plotting the bar chart for missing percentages\n",
        "    missing_percentage.sort_values().plot(kind='barh', color='skyblue', edgecolor='grey')\n",
        "    plt.title(f'Missing Data Percentage - {dataset_name}', fontsize=16)\n",
        "    plt.xlabel('Missing Percentage (%)', fontsize=12)\n",
        "    plt.ylabel('Columns', fontsize=12)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    missing_plot_path = f'/content/5.1 Original Data Missing PercentPlot.png'     # Path to save the plot\n",
        "    plt.savefig(missing_plot_path)\n",
        "    plt.close()                                                                   # Close the plot to avoid displaying it again\n",
        "\n",
        "    return missing_plot_path\n",
        "\n",
        "def get_suggestions(null_counts):                                                 # Function to generate suggestions based on Null values\n",
        "    suggestions = []\n",
        "\n",
        "    no_null_columns = null_counts[null_counts == 0].index.tolist()                # Handle columns with no Null values\n",
        "    if no_null_columns:\n",
        "        suggestions.append(f\"<div style='color:green;'><strong>Columns {', '.join(no_null_columns)}</strong> have no Null values. <em>No action is needed.</em></div>\")\n",
        "\n",
        "    columns_with_null_data = null_counts[null_counts > 0].index.tolist()          # Handle columns with Null values\n",
        "    if columns_with_null_data:\n",
        "        suggestions.append(f\"<div style='color:red;'><strong>Columns {', '.join(columns_with_null_data)}</strong> have Null values. <em>Actions should be taken.</em></div>\")\n",
        "\n",
        "    return suggestions\n",
        "\n",
        "def display_summary_for_file(df, title):                                          # Function to display the summary table and suggestions\n",
        "    summary_table, null_counts, null_percentages = prepare_summary_table(df)      # Prepare the summary table\n",
        "    summary_html = generate_summary_html(summary_table)\n",
        "\n",
        "    suggestions = get_suggestions(null_counts)                                    # Get the suggestions based on Null values handling\n",
        "\n",
        "    missing_plot_path = plot_missing_percentage(df, title)                        # Plot the missing percentage visual and get the path of the image\n",
        "                                                                                  # Combine only the table and suggestions into the HTML content\n",
        "    complete_html = f\"\"\"\n",
        "    <html>\n",
        "    <head><title>Dataset Summary - {title}</title></head>\n",
        "    <body>\n",
        "        <h2>Summary Table for {title}</h2>\n",
        "        {summary_html}\n",
        "        <h2>Suggestions</h2>\n",
        "        {\"\".join(suggestions)}\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    summary_html_path = f'/content/5.1 Learning Application Domain.html'          # Save the HTML output to a file for table + suggestions\n",
        "    with open(summary_html_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(complete_html)\n",
        "\n",
        "    return summary_html_path, missing_plot_path\n",
        "\n",
        "html_file_path, missing_plot_path = display_summary_for_file(df, \"Analysis on Original Data\") # Save and display summary for dataset\n",
        "\n",
        "display(HTML(f\"<h2>Summary Table and Suggestions for Major Crime Indicators Dataset</h2><br>{open(html_file_path).read()}\"))  # Display the summary table and suggestions in the console\n",
        "\n",
        "files.download(html_file_path)                                                    # Download the generated Summary Table + Suggestions as HTML\n",
        "files.download(missing_plot_path)                                                 # Download the generated Missing Data Percentage plot as PNG\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U52Qg2r9t8Ww",
        "outputId": "69fe44e6-dd57-47b5-b4fc-983e26e3e340"
      },
      "id": "U52Qg2r9t8Ww",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/mohammadbadi/crimes-in-toronto?dataset_version_number=1&file_name=major-crime-indicators.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117M/117M [00:03<00:00, 33.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.2\tCreating Target Dataset**"
      ],
      "metadata": {
        "id": "js3f9Rn2uAVB"
      },
      "id": "js3f9Rn2uAVB"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings                                                                   # Import necessary libraries\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import contextlib\n",
        "from google.colab import files\n",
        "from IPython.display import display, HTML\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)                    # Ignore Deprecation Warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)                         # Ignore future warnings\n",
        "\n",
        "file_path = \"major-crime-indicators.csv\"                                          # Set the file path to the filename with extension\n",
        "\n",
        "crime_df = kagglehub.load_dataset(                                                # Load the latest version of the dataset from Kaggle\n",
        "    kagglehub.KaggleDatasetAdapter.PANDAS,\n",
        "    \"mohammadbadi/crimes-in-toronto\",                                             # Updated dataset handle\n",
        "    file_path,\n",
        ")\n",
        "\n",
        "initial_count = df.shape[0]\n",
        "\n",
        "filter1_df = df[(df['UCR_CODE'] == 2135) & (df['UCR_EXT'] == 210)].copy()         # Filter 1: UCR Code 2135 with UCR Extension 210\n",
        "count1 = filter1_df.shape[0]\n",
        "\n",
        "filter2_df = df[(df['UCR_CODE'] == 1610) & (df['UCR_EXT'] == 140)].copy()         # Filter 2: UCR Code 1610 with UCR Extension 140\n",
        "count2 = filter2_df.shape[0]\n",
        "\n",
        "final_df = pd.concat([filter1_df, filter2_df]).copy()                             # Final dataset: Union of both filters\n",
        "final_count = final_df.shape[0]\n",
        "\n",
        "final_df.to_csv('Target_Dataset.csv', index=False)                                # Save the final dataset as Target_Dataset.csv\n",
        "files.download('Target_Dataset.csv')                                              # Download the saved dataset csv\n",
        "\n",
        "steps_summary = []                                                                # Build steps summary as a list of dictionaries\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Filter 1: UCR Code 2135 with UCR Extension 210\",\n",
        "    \"Before Action\": initial_count,\n",
        "    \"Affected by Action\": count1,\n",
        "    \"After Action\": count1,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Filter 2: UCR Code 1610 with UCR Extension 140\",\n",
        "    \"Before Action\": count1,                                                      # using count1 as the 'After Action' of Filter 1\n",
        "    \"Affected by Action\": count2,\n",
        "    \"After Action\": final_count,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Rows Affected in <strong>UCR Filtering</strong>\",\n",
        "    \"Before Action\": \"Initial Load:<br><strong>\" + str(initial_count) + \"</strong>\",\n",
        "    \"Affected by Action\": \"Rows Filtered:<br><strong>\" + str(initial_count - final_count) + \"</strong>\",\n",
        "    \"After Action\": \"Final Count:<br><strong>\" + str(final_count) + \"</strong>\",\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "html_output_filename = '/content/5.2 Target Dataset.html'                         # Set the HTML output filename\n",
        "                                                                                  # Create HTML Table with styling\n",
        "html_table = \"\"\"\n",
        "<table style='border-collapse: collapse; width: 100%; font-size: 18px;'>\n",
        "    <thead style='background-color: #4CAF50; color: white;'>\n",
        "        <tr>\n",
        "            <th colspan=\"5\" style=\"text-align: center; font-size: 24px; background-color: #2f4f4f; color: white;\">\n",
        "                5.2 Creating Target Dataset\n",
        "            </th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <th>Step Taken</th>\n",
        "            <th>Before Action</th>\n",
        "            <th>Affected by Action</th>\n",
        "            <th>After Action</th>\n",
        "            <th>Unit</th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "\"\"\"\n",
        "\n",
        "for step in steps_summary:\n",
        "    html_table += f\"\"\"\n",
        "    <tr style='border: 1px solid #dddddd;'>\n",
        "        <td style='border: 1px solid #dddddd; padding: 8px;'>{step['Step Taken']}</td>\n",
        "        <td style='border: 1px solid #dddddd; padding: 8px;'>{step['Before Action']}</td>\n",
        "        <td style='border: 1px solid #dddddd; padding: 8px;'>{step['Affected by Action']}</td>\n",
        "        <td style='border: 1px solid #dddddd; padding: 8px;'>{step['After Action']}</td>\n",
        "        <td style='border: 1px solid #dddddd; padding: 8px;'>{step['Unit']}</td>\n",
        "    </tr>\n",
        "    \"\"\"\n",
        "\n",
        "note_text = (                                                                     # Add a final row with the note (spanning all columns)\n",
        "    \"<strong>Note: The dataset contains \"\n",
        "    \"<span style='color: darkred; '>ALL CRIMES</span>, but our research focuses on \"\n",
        "    \"<span style='color: green; '>MOTOR VEHICLE THEFTS</span>. \"\n",
        "    \"Therefore, we applied two filters: <br>\"\n",
        "    \"• Filter 1: UCR Code 2135 with UCR Extension 210 for Theft of a Motor Vehicle (Auto Theft), and <br>\"\n",
        "    \"• Filter 2: UCR Code 1610 with UCR Extension 140 for Robbery - Vehicle Jacking.<br>\"\n",
        "    \"The target dataset has been saved as <span style='color: blue;'>'Target_Dataset.csv'</span> for further analysis. </strong>\"\n",
        ")\n",
        "html_table += f\"\"\"\n",
        "    <tr style='border: 1px solid #dddddd;'>\n",
        "        <td colspan=\"5\" style='border: 1px solid #dddddd; padding: 8px;'>{note_text}</td>\n",
        "    </tr>\n",
        "\"\"\"\n",
        "html_table += \"</tbody></table>\"\n",
        "\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "display(HTML(html_table))                                                          # Display the output HTML table\n",
        "\n",
        "with open(html_output_filename, 'w', encoding='utf-8') as f:\n",
        "    f.write(html_table)\n",
        "files.download(html_output_filename)\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "PbXvYjXduDqw"
      },
      "id": "PbXvYjXduDqw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.3 Data Cleaning**"
      ],
      "metadata": {
        "id": "Z0fqcDzmuHV4"
      },
      "id": "Z0fqcDzmuHV4"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings                                                                   # Import necessary libraries\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)                    # Ignore Deprecation Warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)                         # Ignore future warnings\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/mohammadbadi/CrimeAnalytics_Clustering_Approach_3/refs/heads/main/Output_CSV/Target_Dataset.csv\"    # Read the data from CSV file\n",
        "Data_Preparing_df = pd.read_csv(url, low_memory=False).copy()\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "html_output_filename = '/content/5.3 Data Cleaning.html'                          # Output File Name for HTML summary changed to \"5.3 Data Cleaning\"\n",
        "\n",
        "steps_summary = []                                                                # Table to store results\n",
        "\n",
        "before_step_1 = Data_Preparing_df.shape[0]                                        # Step 1: Dataset Loading\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 1: Load Dataset\",\n",
        "    \"Before Action\": before_step_1,\n",
        "    \"Affected by Action\": 0,\n",
        "    \"After Action\": before_step_1,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "columns_to_check = [col for col in Data_Preparing_df.columns if col != '_id']     # Step 2: Identify and remove true duplicates (excluding '_id')\n",
        "duplicate_count = Data_Preparing_df.duplicated(subset=columns_to_check).sum()\n",
        "rows_before_dedup = Data_Preparing_df.shape[0]\n",
        "Data_Preparing_df = Data_Preparing_df.drop_duplicates(subset=columns_to_check, keep='first').copy()\n",
        "rows_after_dedup = Data_Preparing_df.shape[0]\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 2: Remove TRUE DUPLICATE Records\",\n",
        "    \"Before Action\": rows_before_dedup,\n",
        "    \"Affected by Action\": duplicate_count,\n",
        "    \"After Action\": rows_after_dedup,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "before_drop_rows = Data_Preparing_df.shape[0]                                     # Step 3: Drop rows with null, NaN, or missing data\n",
        "Data_Preparing_df = Data_Preparing_df.dropna().copy()\n",
        "after_drop_rows = Data_Preparing_df.shape[0]\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 3: Drop Rows with Missing Data\",\n",
        "    \"Before Action\": before_drop_rows,\n",
        "    \"Affected by Action\": before_drop_rows - after_drop_rows,\n",
        "    \"After Action\": after_drop_rows,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "obj_cols = Data_Preparing_df.select_dtypes(include=\"object\").columns              # Step 4: Strip leading and trailing spaces from string columns\n",
        "orig_step4 = Data_Preparing_df[obj_cols].copy()\n",
        "Data_Preparing_df[obj_cols] = Data_Preparing_df[obj_cols].apply(lambda s: s.str.strip())\n",
        "affected_step4 = (orig_step4 != Data_Preparing_df[obj_cols]).any(axis=1).sum()\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 4: Strip Leading/Trailing Spaces\",\n",
        "    \"Before Action\": after_drop_rows,\n",
        "    \"Affected by Action\": affected_step4,\n",
        "    \"After Action\": after_drop_rows,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "orig_step5 = Data_Preparing_df[obj_cols].copy()                                   # Step 5: Remove leading apostrophes from string columns\n",
        "Data_Preparing_df[obj_cols] = Data_Preparing_df[obj_cols].apply(lambda s: s.str.lstrip(\"'\"))\n",
        "affected_step5 = (orig_step5 != Data_Preparing_df[obj_cols]).any(axis=1).sum()\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 5: Remove Leading Apostrophes\",\n",
        "    \"Before Action\": after_drop_rows,\n",
        "    \"Affected by Action\": affected_step5,\n",
        "    \"After Action\": after_drop_rows,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "nsa_replaced_count = 0                                                            # Step 6: Match rows where 'HOOD_158' is 'NSA' and replace with matching value based on coordinates\n",
        "for i, row in Data_Preparing_df[Data_Preparing_df['HOOD_158'] == 'NSA'].iterrows():\n",
        "    match = Data_Preparing_df[\n",
        "        (Data_Preparing_df['LONG_WGS84'] == row['LONG_WGS84']) &\n",
        "        (Data_Preparing_df['LAT_WGS84'] == row['LAT_WGS84']) &\n",
        "        (Data_Preparing_df['HOOD_158'] != 'NSA')\n",
        "    ]\n",
        "    if not match.empty:\n",
        "        matched_value = match.iloc[0]['HOOD_158']\n",
        "        Data_Preparing_df.loc[i, 'HOOD_158'] = matched_value\n",
        "        nsa_replaced_count += 1\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 6: Match & Replace 'NSA' Values\",\n",
        "    \"Before Action\": after_drop_rows,\n",
        "    \"Affected by Action\": nsa_replaced_count,\n",
        "    \"After Action\": after_drop_rows,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "mask_remaining_nsa = (Data_Preparing_df['HOOD_158'] == 'NSA') | (Data_Preparing_df['NEIGHBOURHOOD_158'] == 'NSA')     # Step 7: Remove remaining rows where 'HOOD_158' or 'NEIGHBOURHOOD_158' contains 'NSA'\n",
        "remaining_nsa_count = mask_remaining_nsa.sum()\n",
        "before_removal = Data_Preparing_df.shape[0]\n",
        "Data_Preparing_df = Data_Preparing_df[~mask_remaining_nsa].copy()\n",
        "after_removal = Data_Preparing_df.shape[0]\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 7: Remove Remaining 'NSA' Rows\",\n",
        "    \"Before Action\": before_removal,\n",
        "    \"Affected by Action\": remaining_nsa_count,\n",
        "    \"After Action\": after_removal,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "orig_long = Data_Preparing_df['LONG_WGS84'].copy()                                # Step 8: Format Longitude & Latitude to 7 decimals\n",
        "orig_lat = Data_Preparing_df['LAT_WGS84'].copy()\n",
        "Data_Preparing_df.loc[:, 'LONG_WGS84'] = Data_Preparing_df['LONG_WGS84'].astype(float).map(lambda x: f\"{x:.7f}\")\n",
        "Data_Preparing_df.loc[:, 'LAT_WGS84'] = Data_Preparing_df['LAT_WGS84'].astype(float).map(lambda x: f\"{x:.7f}\")\n",
        "affected_step8 = ((orig_long != Data_Preparing_df['LONG_WGS84']) | (orig_lat != Data_Preparing_df['LAT_WGS84'])).sum()\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 8: Format Longitude & Latitude to 7 Decimals\",\n",
        "    \"Before Action\": after_removal,\n",
        "    \"Affected by Action\": affected_step8,\n",
        "    \"After Action\": after_removal,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "final_row_count = Data_Preparing_df.shape[0]                                      # Final row: Rows Affected in 5.3 Data Cleaning\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Rows Affected in <strong>5.3 Data Cleaning </strong>\",\n",
        "    \"Before Action\": \"Initial Load:<br><strong>\" + str(before_step_1) + \"</strong>\",\n",
        "    \"Affected by Action\": \"Overall Reduction:<br><strong>\" + str(before_step_1 - final_row_count) + \"</strong>\",\n",
        "    \"After Action\": \"Final Count:<br><strong>\" + str(final_row_count) + \"</strong>\",\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "                                                                                  # Build HTML Table with styling\n",
        "html_table = \"\"\"\n",
        "<table style='border-collapse: collapse; width: 100%; font-size: 18px;'>\n",
        "    <thead style='background-color: #4CAF50; color: white;'>\n",
        "        <tr>\n",
        "            <th colspan=\"5\" style=\"text-align: center; font-size: 24px; background-color: #2f4f4f; color: white;\">\n",
        "                5.3 Data Cleaning\n",
        "            </th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <th>Step Taken</th>\n",
        "            <th>Before Action</th>\n",
        "            <th>Affected by Action</th>\n",
        "            <th>After Action</th>\n",
        "            <th>Unit</th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "\"\"\"\n",
        "for step in steps_summary:\n",
        "    html_table += f\"\"\"\n",
        "        <tr style='border: 1px solid #dddddd;'>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px;'>{step['Step Taken']}</td>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px;'>{step['Before Action']}</td>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px;'>{step['Affected by Action']}</td>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px;'>{step['After Action']}</td>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px;'>{step['Unit']}</td>\n",
        "        </tr>\n",
        "    \"\"\"\n",
        "note_text = (                                                                     # Add final note row spanning all columns with the required note text\n",
        "    \"<strong>Note: \"\n",
        "    \"Longitude and Latitude were reduced to 7 Decimal Places as,<br>\"\n",
        "    \"• 7 decimal places offer precision of <span style='color: green;'>1.1 cm</span>, precise enough for GPS devices.<br>\"\n",
        "    \"• Further granularity adds processing time and energy consumption without real-world benefits.<br>\"\n",
        "    \"The final cleaned data has been saved as <span style='color: blue;'> 'Cleaned_Data.csv' </span> for further analysis.</strong>\"\n",
        ")\n",
        "html_table += f\"\"\"\n",
        "        <tr style='border: 1px solid #dddddd;'>\n",
        "            <td colspan=\"5\" style='border: 1px solid #dddddd; padding: 8px;'>{note_text}</td>\n",
        "        </tr>\n",
        "    </tbody>\n",
        "</table>\n",
        "\"\"\"\n",
        "\n",
        "Data_Preparing_df.to_csv('Cleaned_Data.csv', index=False)                         # Save the cleaned data to a new CSV file\n",
        "\n",
        "display(HTML(html_table))                                                         # Display the HTML table\n",
        "\n",
        "with open(html_output_filename, 'w', encoding='utf-8') as f:                      # Save the HTML table to a file\n",
        "    f.write(html_table)\n",
        "files.download(html_output_filename)                                              # Download the HTML file\n",
        "files.download('Cleaned_Data.csv')                                                # Download the cleaned CSV file\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "67adE3PfuIjQ"
      },
      "id": "67adE3PfuIjQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.4\tData Reduction and Projection - a)\tFeature Engineering, b) EDA, c) Feature Engineering & d) Data Reduction**"
      ],
      "metadata": {
        "id": "dnNcFxi1vFm7"
      },
      "id": "dnNcFxi1vFm7"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings                                                                   # Import necessary libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import BoundaryNorm, LinearSegmentedColormap\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)                    # Ignore Deprecation Warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)                         # Ignore future warnings\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/mohammadbadi/CrimeAnalytics_Clustering_Approach_3/refs/heads/main/Output_CSV/Cleaned_Data.csv\"    # Read the data from CSV file\n",
        "df = pd.read_csv(url, parse_dates=['REPORT_DATE', 'OCC_DATE'], low_memory=False)\n",
        "\n",
        "df['DAYS_DIFFERENCE'] = (df['REPORT_DATE'] - df['OCC_DATE']).dt.days              # Calculate the difference in days between REPORT_DATE and OCC_DATE\n",
        "df = df[(df['DAYS_DIFFERENCE'] >= 0) & (df['DAYS_DIFFERENCE'] <= 365)]            # Filter out invalid values (0 to 365 days)\n",
        "day_diff_counts = df['DAYS_DIFFERENCE'].value_counts().reset_index()              # Count occurrences of each 'DAYS_DIFFERENCE'\n",
        "day_diff_counts.columns = ['Day #', 'Reported number']\n",
        "day_diff_counts = day_diff_counts.sort_values(by='Day #')\n",
        "\n",
        "days_range = pd.DataFrame({'Day #': range(0, 90)})                                # Create a DataFrame for days 0 to 89 (90 days total so they can be arranged in 30 rows x 3 columns)\n",
        "table_df = pd.merge(days_range, day_diff_counts, on='Day #', how='left')\n",
        "table_df['Reported number'] = table_df['Reported number'].fillna(0).astype(int)\n",
        "\n",
        "total_incidents = table_df['Reported number'].sum()                               # Calculate totals and percentages\n",
        "table_df['% Report'] = table_df['Reported number'] / total_incidents * 100\n",
        "table_df['Total reported until now'] = table_df['Reported number'].cumsum()\n",
        "table_df['Total % Reported'] = table_df['Total reported until now'] / total_incidents * 100\n",
        "\n",
        "table_df['Reported number'] = table_df['Reported number'].map(lambda x: f\"{x:>10d}\")    # Format numeric columns with right-justification\n",
        "table_df['% Report'] = table_df['% Report'].map(lambda x: f\"{x:>6.2f}%\")\n",
        "table_df['Total reported until now'] = table_df['Total reported until now'].map(lambda x: f\"{x:>10d}\")\n",
        "table_df['Total % Reported'] = table_df['Total % Reported'].map(lambda x: f\"{x:>6.2f}%\")\n",
        "\n",
        "n_cols = 3                                                                        # Reshape the DataFrame into a grid with 3 columns and 30 rows.\n",
        "n_rows = len(table_df) // n_cols                                                  # Should be 30 rows if table_df has 90 rows\n",
        "\n",
        "records = table_df.to_dict('records')\n",
        "grid = []\n",
        "for i in range(n_rows):\n",
        "    row_records = []\n",
        "    for j in range(n_cols):\n",
        "        idx = j * n_rows + i\n",
        "        row_records.append(records[idx])\n",
        "    grid.append(row_records)\n",
        "\n",
        "                                                                                  # Build the HTML table.\n",
        "html_table = \"\"\"\n",
        "<table style='border-collapse: collapse; width: 100%; font-size: 18px;'>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th colspan=\"15\" style=\"text-align: center; font-size: 24px; background-color: #2f4f4f; color: white; padding: 8px;\">\n",
        "        Distribution of Reporting Delays (30 rows x 3 columns)\n",
        "      </th>\n",
        "    </tr>\n",
        "    <tr style='background-color: #4CAF50; color: white;'>\n",
        "      <th><strong>Day #</strong></th>\n",
        "      <th>Reported number</th>\n",
        "      <th>% Report</th>\n",
        "      <th>Total reported until now</th>\n",
        "      <th>Total % Reported</th>\n",
        "      <th style='border-left: 4px solid #dddddd;'><strong>Day #</strong></th>\n",
        "      <th>Reported number</th>\n",
        "      <th>% Report</th>\n",
        "      <th>Total reported until now</th>\n",
        "      <th>Total % Reported</th>\n",
        "      <th style='border-left: 4px solid #dddddd;'><strong>Day #</strong></th>\n",
        "      <th>Reported number</th>\n",
        "      <th>% Report</th>\n",
        "      <th>Total reported until now</th>\n",
        "      <th>Total % Reported</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "\"\"\"\n",
        "\n",
        "for row in grid:\n",
        "    html_table += \"<tr>\"\n",
        "    for j, record in enumerate(row):\n",
        "        if j > 0:                                                                 # For the Day # cell, add bold formatting.\n",
        "            day_style = \"border: 1px solid #dddddd; padding: 8px; text-align: right; border-left: 4px solid #dddddd; \"\n",
        "        else:\n",
        "            day_style = \"border: 1px solid #dddddd; padding: 8px; text-align: right; \"\n",
        "        html_table += f\"<td style='{day_style}'><strong>{record['Day #']}</strong></td>\"\n",
        "        html_table += f\"<td style='border: 1px solid #dddddd; padding: 8px; text-align: right;'>{record['Reported number']}</td>\"\n",
        "        html_table += f\"<td style='border: 1px solid #dddddd; padding: 8px; text-align: right;'>{record['% Report']}</td>\"\n",
        "        html_table += f\"<td style='border: 1px solid #dddddd; padding: 8px; text-align: right;'>{record['Total reported until now']}</td>\"\n",
        "        html_table += f\"<td style='border: 1px solid #dddddd; padding: 8px; text-align: right;'>{record['Total % Reported']}</td>\"\n",
        "    html_table += \"</tr>\"\n",
        "html_table += \"\"\"\n",
        "  </tbody>\n",
        "</table>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(html_table))\n",
        "\n",
        "\n",
        "day_diff_counts = df['DAYS_DIFFERENCE'].value_counts().reset_index()              # Count occurrences of each 'DAYS_DIFFERENCE'\n",
        "day_diff_counts.columns = ['Days Difference', 'Occurrences']\n",
        "day_diff_counts = day_diff_counts.sort_values(by='Days Difference')\n",
        "\n",
        "day_diff_filtered = day_diff_counts[day_diff_counts['Days Difference'] <= 50].copy()  # Filter for days <= 50 for the first chart\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))                                           #First Chart: Bar Plot (First 50 Days)\n",
        "sns.barplot(x='Days Difference', y='Occurrences', data=day_diff_filtered, palette='viridis', ax=ax)\n",
        "ax.set_title('Distribution of Days Difference (First 50 Days)', fontsize=16)\n",
        "ax.set_xlabel('Days Difference', fontsize=14)\n",
        "ax.set_ylabel('Occurrences', fontsize=14)\n",
        "ax.set_xlim(-1, 50)\n",
        "ax.set_xticks(day_diff_filtered[\"Days Difference\"])                               # Set tick positions explicitly\n",
        "ax.set_xticklabels(day_diff_filtered[\"Days Difference\"], rotation=45)             # Then set tick labels\n",
        "\n",
        "total_incidents = day_diff_counts['Occurrences'].sum()                            # Compute totals for day 0 and day 1 based on the complete dataset\n",
        "zero_day_count = (day_diff_counts[day_diff_counts['Days Difference'] == 0]['Occurrences']\n",
        "                  .sum() if not day_diff_counts[day_diff_counts['Days Difference'] == 0].empty else 0)\n",
        "one_day_count = (day_diff_counts[day_diff_counts['Days Difference'] == 1]['Occurrences']\n",
        "                 .sum() if not day_diff_counts[day_diff_counts['Days Difference'] == 1].empty else 0)\n",
        "total_zero_one = zero_day_count + one_day_count\n",
        "\n",
        "percent_zero = (zero_day_count / total_incidents) * 100 if total_incidents > 0 else 0   # Calculate percentages\n",
        "percent_one = (one_day_count / total_incidents) * 100 if total_incidents > 0 else 0\n",
        "percent_total = (total_zero_one / total_incidents) * 100 if total_incidents > 0 else 0\n",
        "\n",
        "explanation_text = (                                                              # Explanation text with three lines; construct the mathtext for the percentages separately so that only those values are bold.\n",
        "    f\"crimes reported by same Day: {zero_day_count} which is \" +\n",
        "    \"$\\\\mathbf{\" + f\"{percent_zero:.2f}\" + \"\\\\%}$\" + \"\\n\\n\" +\n",
        "    f\"crimes reported within 1 Day: {one_day_count} which is \" +\n",
        "    \"$\\\\mathbf{\" + f\"{percent_one:.2f}\" + \"\\\\%}$\" + \"\\n\\n\" +\n",
        "    f\"Total crimes reported within 1 day: {total_zero_one} which is \" +\n",
        "    \"$\\\\mathbf{\" + f\"{percent_total:.2f}\" + \"\\\\%}$\"\n",
        ")\n",
        "\n",
        "ax.text(0.97, 0.96, explanation_text, transform=ax.transAxes, ha='right', va='top', # Place the explanation text inside the graph at the top-right using axis coordinates\n",
        "        fontsize=12, bbox=dict(facecolor='white', alpha=0.7))\n",
        "\n",
        "bar_plot_filename = '/content/5.4 EDA_First_50days.png'                                # Save and download the chart\n",
        "plt.savefig(bar_plot_filename, bbox_inches='tight', dpi=300)\n",
        "plt.show()\n",
        "files.download(bar_plot_filename)\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "Data_Processing_df = pd.read_csv(url, low_memory=False).copy()\n",
        "\n",
        "html_output_filename = \"/content/5.4 Data Reduction and Projection.html\"\n",
        "\n",
        "steps_summary = []                                                                # Store processing steps summary\n",
        "\n",
        "before_step_1 = Data_Processing_df.shape[0]                                       # Step 1: Dataset Loading\n",
        "before_columns = Data_Processing_df.shape[1]                                      # Columns at the time of loading dataset\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 1: Load Dataset\",\n",
        "    \"Before Action\": before_step_1,\n",
        "    \"Affected by Action\": \"\",\n",
        "    \"After Action\": before_step_1,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "current_rows = Data_Processing_df.shape[0]                                        # Use current row count after loading as reference for subsequent steps\n",
        "\n",
        "Data_Processing_df['REPORT_DATETIME'] = pd.to_datetime(Data_Processing_df['REPORT_DATE']) + pd.to_timedelta(Data_Processing_df['REPORT_HOUR'], unit='h')    # Step 2: Create datetime columns for 'REPORT_DATETIME' and 'OCC_DATETIME'\n",
        "Data_Processing_df['OCC_DATETIME'] = pd.to_datetime(Data_Processing_df['OCC_DATE']) + pd.to_timedelta(Data_Processing_df['OCC_HOUR'], unit='h')\n",
        "\n",
        "rows_after_datetime = Data_Processing_df.shape[0]                                 # The row count remains unchanged after creating datetime columns\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 2: Feature Engineering - Join the Date & Time\",\n",
        "    \"Before Action\": current_rows,\n",
        "    \"Affected by Action\": \"New Columns Added\",\n",
        "    \"After Action\": rows_after_datetime,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "reporting_timedelta = Data_Processing_df['REPORT_DATETIME'] - Data_Processing_df['OCC_DATETIME']  # Step 3: Calculate the reporting delay (days + hours only)\n",
        "Data_Processing_df['reporting_delay_days'] = reporting_timedelta.dt.days\n",
        "Data_Processing_df['reporting_delay_hours'] = (reporting_timedelta.dt.seconds // 3600)  # Convert seconds to full hours\n",
        "\n",
        "rows_after_delay_calc = Data_Processing_df.shape[0]                               # The row count remains unchanged after calculating delays\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 3: Feature Engineering: Compute Reporting Delay in Days & Hours\",\n",
        "    \"Before Action\": rows_after_datetime,\n",
        "    \"Affected by Action\": \"New Columns Added\",\n",
        "    \"After Action\": rows_after_delay_calc,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "before_delay_filter = Data_Processing_df.shape[0]                                 # Step 4: Filter records with reporting delays between 0 to 60 days\n",
        "Data_Processing_df = Data_Processing_df[\n",
        "    (Data_Processing_df['reporting_delay_days'] >= 0) &\n",
        "    (Data_Processing_df['reporting_delay_days'] <= 1)\n",
        "].copy()\n",
        "after_delay_filter = Data_Processing_df.shape[0]\n",
        "delay_filtered_rows = before_delay_filter - after_delay_filter\n",
        "\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 4: Only Keep Complaints with 0-60 Days Delay\",\n",
        "    \"Before Action\": before_delay_filter,\n",
        "    \"Affected by Action\": delay_filtered_rows,\n",
        "    \"After Action\": after_delay_filter,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "columns_to_drop = ['REPORT_YEAR', 'REPORT_MONTH', 'REPORT_DAY', 'REPORT_DOY', 'REPORT_DOW', # Step 5: Drop unnecessary columns\n",
        "                   'HOOD_140', 'NEIGHBOURHOOD_140', 'UCR_CODE', 'UCR_EXT', 'OFFENCE', 'MCI_CATEGORY',\n",
        "                   'REPORT_DATE', 'OCC_DATE', 'REPORT_HOUR', 'REPORT_DATETIME']\n",
        "columns_before_drop = Data_Processing_df.shape[1]\n",
        "\n",
        "dropped_column_names = [col for col in columns_to_drop if col in Data_Processing_df.columns]  # Store column names before dropping\n",
        "\n",
        "Data_Processing_df = Data_Processing_df.drop(columns=dropped_column_names).copy() # Drop columns\n",
        "columns_after_drop = Data_Processing_df.shape[1]\n",
        "dropped_columns = columns_before_drop - columns_after_drop\n",
        "\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 5: Dropped Unnecessary Columns\",\n",
        "    \"Before Action\": columns_before_drop,\n",
        "    \"Affected by Action\": dropped_columns,\n",
        "    \"After Action\": columns_after_drop,\n",
        "    \"Unit\": \"Columns\"\n",
        "})\n",
        "\n",
        "Data_Processing_df.to_csv('Final_Data.csv', index=False)                          # Save the final data to a CSV file\n",
        "\n",
        "steps_summary.append({                                                            # Step 8: Summary - Rows Affected\n",
        "    \"Step Taken\": \"Rows Affected in <strong>5.4 Data Reduction and Projection</strong>\",\n",
        "    \"Before Action\": f\"Initial Load:<br><strong>{before_step_1}</strong>\",\n",
        "    \"Affected by Action\": f\"Rows Filtered:<br><strong>{before_step_1 - after_delay_filter}</strong>\",\n",
        "    \"After Action\": f\"Final Count: <br><strong>{after_delay_filter}</strong>\",\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "steps_summary.append({                                                            # Step 9: Summary - Columns Affected\n",
        "    \"Step Taken\": \"Columns Affected in <strong>5.4 Data Reduction and Projection</strong>\",\n",
        "    \"Before Action\": f\"Initial Load:<br><strong>{before_columns}</strong>\",\n",
        "    \"Affected by Action\": f\"Columns Dropped:<br><strong>{dropped_columns}</strong>\",\n",
        "    \"After Action\": f\"Final Count: <br><strong>{columns_after_drop}</strong>\",\n",
        "    \"Unit\": \"Columns\"\n",
        "})\n",
        "                                                                                  # Create HTML Table with styling\n",
        "html_table = \"\"\"\n",
        "<table style='border-collapse: collapse; width: 100%; font-size: 18px;'>\n",
        "    <thead style='background-color: #4CAF50; color: white;'>\n",
        "        <tr>\n",
        "            <th colspan=\"5\" style=\"text-align: center; font-size: 24px; background-color: #2f4f4f; color: white;\">\n",
        "                <strong>5.4 Data Reduction and Projection</strong>\n",
        "            </th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <th>Step Taken</th>\n",
        "            <th>Before Action</th>\n",
        "            <th>Affected by Action</th>\n",
        "            <th>After Action</th>\n",
        "            <th>Unit</th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "\"\"\"\n",
        "\n",
        "for step in steps_summary:\n",
        "    html_table += f\"\"\"\n",
        "    <tr style='border: 1px solid #dddddd;'>\n",
        "        <td style='border: 1px solid #dddddd; padding: 8px;'>{step['Step Taken']}</td>\n",
        "        <td style='border: 1px solid #dddddd; padding: 8px;'>{step['Before Action']}</td>\n",
        "        <td style='border: 1px solid #dddddd; padding: 8px;'>{step['Affected by Action']}</td>\n",
        "        <td style='border: 1px solid #dddddd; padding: 8px;'>{step['After Action']}</td>\n",
        "        <td style='border: 1px solid #dddddd; padding: 8px;'>{step['Unit']}</td>\n",
        "    </tr>\n",
        "    \"\"\"\n",
        "html_table += \"</tbody></table>\"\n",
        "\n",
        "                                                                                  # Final message about saved data\n",
        "final_message = \"\"\"\n",
        "<div style=\"font-size: 18px; color: #333; font-weight: bold; padding: 10px;\">\n",
        "    The final data after Data Reduction and Projection has been saved as <span style=\"color: blue;\">'Final_Data.csv'</span> for further analysis.\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(html_table))                                                         # Display the tables\n",
        "display(HTML(final_message))                                                      # Display the final message\n",
        "\n",
        "with open(html_output_filename, 'w', encoding='utf-8') as f:\n",
        "    f.write(html_table)\n",
        "    f.write(final_message)\n",
        "\n",
        "files.download(html_output_filename)                                              # Download HTML File\n",
        "files.download('Final_Data.csv')                                                  # Download CSV File\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "OoNalt-mvGVX"
      },
      "id": "OoNalt-mvGVX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.4 Data Reduction and Projection - e) EDA**"
      ],
      "metadata": {
        "id": "vL8J_ftgvVdW"
      },
      "id": "vL8J_ftgvVdW"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import BoundaryNorm, LinearSegmentedColormap\n",
        "from scipy import stats\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "                                                                                  # Ignore specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "                                                                                  # Read the dataset from CSV (using the URL)\n",
        "url = \"https://raw.githubusercontent.com/mohammadbadi/CrimeAnalytics_Clustering_Approach_3/refs/heads/main/Output_CSV/Final_Data.csv\"\n",
        "original_df = pd.read_csv(url, low_memory=False)\n",
        "\n",
        "                                                                                  # Create copies for different visualizations\n",
        "df_box = original_df.copy().drop(columns=['_id', 'EVENT_UNIQUE_ID', 'HOOD_158'])\n",
        "df_line = original_df.copy()                                                      # For missing values line chart\n",
        "df_bar = original_df.copy().drop(columns=['_id', 'EVENT_UNIQUE_ID', 'HOOD_158'])\n",
        "df_cat = df_bar.copy()                                                            # For categorical distribution\n",
        "df_geo = original_df.copy()                                                       # For geographic scatter\n",
        "df_corr = original_df.copy()                                                      # For correlation heatmap\n",
        "                                                                                  # 1) Box Plot of Numerical Features\n",
        "print(\"Generating Box Plot for Numerical Features...\")\n",
        "numerical_columns = df_box.select_dtypes(include=['float64', 'int64']).columns\n",
        "ncols = 4                                                                         # 4 box plots per row\n",
        "nrows = (len(numerical_columns) + ncols - 1) // ncols\n",
        "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 2.5 * nrows))\n",
        "axes = axes.flatten()\n",
        "for i, col in enumerate(numerical_columns):\n",
        "    sns.boxplot(x=df_box[col], ax=axes[i], color=\"#8A0000\")\n",
        "    axes[i].set_title(f\"Box Plot: {col}\")\n",
        "    axes[i].tick_params(axis='x', rotation=45)\n",
        "                                                                                  # Remove any unused axes\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "plt.tight_layout()\n",
        "box_plot_filepath = \"EDA_Boxplot_Numerical_Features.png\"\n",
        "plt.savefig(box_plot_filepath)\n",
        "plt.show()\n",
        "files.download(box_plot_filepath)\n",
        "                                                                                  # 2) Line Chart: Percentage of Missing Values per Column\n",
        "print(\"Generating Line Chart for Missing Values per Column...\")\n",
        "missing_percent = df_line.isnull().mean() * 100\n",
        "plt.figure(figsize=(10, 3))\n",
        "missing_percent.plot(kind='line', color='b', marker='o', linestyle='-', linewidth=2)\n",
        "plt.axhline(0, color='black', linestyle='--')\n",
        "plt.title(\"Percentage of Missing Values per Column\", fontsize=16)\n",
        "plt.xlabel(\"Columns\", fontsize=14)\n",
        "plt.ylabel(\"Missing Value (%)\", fontsize=12)\n",
        "plt.xticks(rotation=40)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "missing_values_chart_path = \"EDA_Missing_Values.png\"\n",
        "plt.savefig(missing_values_chart_path)\n",
        "plt.show()\n",
        "files.download(missing_values_chart_path)\n",
        "                                                                                  # 3) Bar Plot: Incidents per Year, Month, Day of the Week\n",
        "print(\"Generating Bar Plots for Incidents per Year, Month, and Day of the Week...\")\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4.5))\n",
        "                                                                                  # Ensure proper data types and ordering\n",
        "df_bar['OCC_YEAR'] = df_bar['OCC_YEAR'].astype(int)\n",
        "df_bar['OCC_MONTH'] = df_bar['OCC_MONTH'].str[:3]                                 # Use first 3 letters\n",
        "df_bar['OCC_DOW'] = df_bar['OCC_DOW'].str[:3]                                     # Use first 3 letters\n",
        "month_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "dow_order = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "df_bar['OCC_MONTH'] = pd.Categorical(df_bar['OCC_MONTH'], categories=month_order, ordered=True)\n",
        "df_bar['OCC_DOW'] = pd.Categorical(df_bar['OCC_DOW'], categories=dow_order, ordered=True)\n",
        "sns.countplot(x='OCC_YEAR', data=df_bar, palette='coolwarm', ax=axes[0]).set_title(\"Incidents Per Year\")\n",
        "sns.countplot(x='OCC_MONTH', data=df_bar, palette='coolwarm', ax=axes[1]).set_title(\"Incidents Per Month\")\n",
        "sns.countplot(x='OCC_DOW', data=df_bar, palette='coolwarm', ax=axes[2]).set_title(\"Incidents Per Day of the Week\")\n",
        "for ax in axes:\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "incidents_path = \"EDA_Incidents_Per_Year_Month_DayOfWeek.png\"\n",
        "plt.tight_layout()\n",
        "plt.savefig(incidents_path)\n",
        "plt.show()\n",
        "files.download(incidents_path)\n",
        "                                                                                  # 4) Bar Plot: Distribution of Categorical Features\n",
        "                                                                                  # (DIVISION, LOCATION_TYPE, PREMISES_TYPE, NEIGHBOURHOOD_158)\n",
        "print(\"Generating Distribution Plots for Categorical Features...\")\n",
        "categorical_columns = ['DIVISION', 'LOCATION_TYPE', 'PREMISES_TYPE', 'NEIGHBOURHOOD_158']\n",
        "fig, axes = plt.subplots(len(categorical_columns) // 2, 2, figsize=(15, 5 * (len(categorical_columns) // 2)))\n",
        "axes = axes.flatten()\n",
        "for idx, col in enumerate(categorical_columns):\n",
        "    print(f\"\\n===== {col} Distribution =====\\n\")\n",
        "    print(df_cat[col].value_counts().to_string())\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "    top_categories = df_cat[col].value_counts().nlargest(10)\n",
        "    sns.countplot(y=df_cat[col], order=top_categories.index, palette=\"viridis\", ax=axes[idx])\n",
        "    axes[idx].set_title(f\"Distribution of {col}\")\n",
        "    labels = [label.get_text()[:15] for label in axes[idx].get_yticklabels()]\n",
        "    axes[idx].set_yticklabels(labels)\n",
        "plt.tight_layout()\n",
        "categorical_dist_path = \"EDA_Categorical_Distribution.png\"\n",
        "plt.savefig(categorical_dist_path)\n",
        "plt.show()\n",
        "files.download(categorical_dist_path)\n",
        "                                                                                  # 5) Colour Coded Scatter Plot (Geographic Distribution)\n",
        "print(\"Generating Colour Coded Scatter Plot for Geographic Distribution...\")\n",
        "df_geo['Geo_Location'] = df_geo['LONG_WGS84'].round(7).astype(str) + \", \" + df_geo['LAT_WGS84'].round(7).astype(str)\n",
        "location_counts = df_geo['Geo_Location'].value_counts()\n",
        "df_geo['Location_Frequency'] = df_geo['Geo_Location'].map(location_counts)\n",
        "boundaries = [0, 30, 250, 350]\n",
        "colors = ['#006400', '#FFFF00', '#FF6666', '#8B0000']\n",
        "cmap = LinearSegmentedColormap.from_list(\"custom_green_yellow_red\", colors, N=256)\n",
        "norm = BoundaryNorm(boundaries, cmap.N)\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.scatterplot(x='LONG_WGS84', y='LAT_WGS84', data=df_geo, hue='Location_Frequency', palette=cmap,\n",
        "                size='Location_Frequency', sizes=(20, 200), alpha=0.6, legend=None,\n",
        "                hue_norm=norm, ax=ax)\n",
        "ax.set_title(\"Geographic Distribution of Incidents (Color Coded by Frequency)\")\n",
        "ax.set_xlabel(\"Longitude\")\n",
        "ax.set_ylabel(\"Latitude\")\n",
        "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "sm.set_array([])\n",
        "fig.colorbar(sm, ax=ax, label=\"Frequency of Incidents\")\n",
        "plt.tight_layout()\n",
        "geo_plot_filepath = \"Geographic_Scatter_Plot.png\"\n",
        "plt.savefig(geo_plot_filepath, dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "files.download(geo_plot_filepath)\n",
        "print(\"\\n===== Top 10 Most Frequent Incident Locations =====\\n\")\n",
        "print(location_counts.head(10).to_string())\n",
        "print(\"=\"*40)\n",
        "\n",
        "                                                                                  # 6) Correlation Heatmap\n",
        "print(\"Generating Correlation Heatmap...\")\n",
        "df_corr_clean = df_corr.drop(columns=['_id', 'EVENT_UNIQUE_ID', 'reporting_delay_days', 'HOOD_158'])\n",
        "df_corr_clean['OCC_MONTH'] = pd.Categorical(df_corr_clean['OCC_MONTH'], categories=[\n",
        "    'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August',\n",
        "    'September', 'October', 'November', 'December'], ordered=False)\n",
        "df_corr_clean['OCC_DOW'] = pd.Categorical(df_corr_clean['OCC_DOW'], categories=[\n",
        "    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], ordered=False)\n",
        "df_numerical = df_corr_clean.select_dtypes(include=[np.number])\n",
        "correlation_matrix = df_numerical.corr()\n",
        "print(\"\\n===== Correlation Matrix =====\")\n",
        "print(correlation_matrix)\n",
        "print(\"=\"*40)\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, cbar=True)\n",
        "plt.title(\"Correlation Matrix (Excluding Specific Columns)\")\n",
        "plt.tight_layout()\n",
        "corr_plot_filepath = \"Correlation_Heatmap.png\"\n",
        "plt.savefig(corr_plot_filepath, dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "files.download(corr_plot_filepath)\n"
      ],
      "metadata": {
        "id": "jzlB49JnvYI-"
      },
      "id": "jzlB49JnvYI-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.4.3 Feature Engineering - Approach_3**"
      ],
      "metadata": {
        "id": "a4lIiy7ovf8v"
      },
      "id": "a4lIiy7ovf8v"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings                                                                   # Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.decomposition import PCA\n",
        "from google.colab import files\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "                                                                                  # URL of the Dataset\n",
        "url = \"https://raw.githubusercontent.com/mohammadbadi/CrimeAnalytics_Clustering_Approach_3/refs/heads/main/Output_CSV/Final_Data.csv\"\n",
        "try:\n",
        "  df = pd.read_csv(url)\n",
        "                                                                                  # Print statement removed, only using HTML display below\n",
        "except Exception as e:\n",
        "  display(HTML(f\"<p style='color: red; font-size: 16px; font-weight: bold;'>Error loading data: {e}</p>\"))\n",
        "  exit()\n",
        "\n",
        "Data_Preparing_df = pd.read_csv(url)\n",
        "display(HTML(\"<p style='color: green; font-size: 16px; font-weight: bold;'>Data loaded successfully.</p>\"))\n",
        "\n",
        "# -------------------- Feature Engineering --------------------\n",
        "# Capture initial column count\n",
        "initial_cols_count = len(Data_Preparing_df.columns)\n",
        "\n",
        "# Define the grouping for location types\n",
        "residential_types = [\n",
        "    'Apartment (Rooming House, Condo)',\n",
        "    'Single Home, House (Attach Garage, Cottage, Mobile)',\n",
        "    'Group Homes (Non-Profit, Halfway House, Social Agency)',\n",
        "    'Community Group Home', 'Retirement Home', 'Nursing Home',\n",
        "    'Private Property Structure (Pool, Shed, Detached Garage)'\n",
        "]\n",
        "public_types = [\n",
        "    'Streets, Roads, Highways (Bicycle Path, Private Road)',\n",
        "    'Open Areas (Lakes, Parks, Rivers)',\n",
        "    \"Other Non Commercial / Corporate Places (Non-Profit, Gov'T, Firehall)\",\n",
        "    'Parking Lots (Apt., Commercial Or Non-Commercial)'\n",
        "]\n",
        "\n",
        "# Work on a copy for feature engineering\n",
        "df = Data_Preparing_df.copy()\n",
        "\n",
        "# Engineer new columns based on pre-defined groups\n",
        "df['Location_Engineered_Residential'] = df['LOCATION_TYPE'].apply(lambda x: 'Residential' if x in residential_types else None)\n",
        "df['Location_Engineered_Public']      = df['LOCATION_TYPE'].apply(lambda x: 'Public' if x in public_types else None)\n",
        "df['Location_Engineered_Other']       = df['LOCATION_TYPE'].apply(lambda x: 'Other' if (x not in residential_types and x not in public_types) else None)\n",
        "\n",
        "# Capture final column count after feature engineering\n",
        "final_cols_count = len(df.columns)\n",
        "\n",
        "# Build steps_summary with 3 engineered rows and one final row for column counts\n",
        "steps_summary = []\n",
        "steps_summary.append({\n",
        "    \"Original Feature\": \"LOCATION_TYPE\",\n",
        "    \"Action Taken\": \"Engineered new column Location_Engineered_Residential\",\n",
        "    \"Rationale\": \"Captures dwelling types in residential areas.\"\n",
        "})\n",
        "steps_summary.append({\n",
        "    \"Original Feature\": \"LOCATION_TYPE\",\n",
        "    \"Action Taken\": \"Engineered new column Location_Engineered_Public\",\n",
        "    \"Rationale\": \"Groups public and community space locations.\"\n",
        "})\n",
        "steps_summary.append({\n",
        "    \"Original Feature\": \"LOCATION_TYPE\",\n",
        "    \"Action Taken\": \"Engineered new column Location_Engineered_Other\",\n",
        "    \"Rationale\": \"Identifies location types that do not fit the primary groups.\"\n",
        "})\n",
        "steps_summary.append({\n",
        "    \"Original Feature\": \"Columns affected in <br><strong>5.4.3 Feature Engineering - Approach_3</strong>\",\n",
        "    \"Action Taken\": \"Initial Columns: <strong><br>\" + str(initial_cols_count) + \"</strong>\",\n",
        "    \"Rationale\": \"Final Columns: <strong><br>\" + str(final_cols_count) + \"</strong>\"\n",
        "})\n",
        "\n",
        "# Build HTML Table for Feature Engineering Phase with alternate row shading\n",
        "html_table = \"\"\"\n",
        "<table style='border-collapse: collapse; width: 100%; font-size: 18px;'>\n",
        "    <thead style='background-color: #4CAF50; color: white;'>\n",
        "        <tr>\n",
        "            <th colspan=\"3\" style=\"text-align: center; font-size: 24px; background-color: #2f4f4f; color: white;\">\n",
        "                5.4.3 Feature Engineering Phase - Approach_3\n",
        "            </th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <th style='border: 1px solid #dddddd; padding: 8px;'>Original Feature</th>\n",
        "            <th style='border: 1px solid #dddddd; padding: 8px;'>Action Taken</th>\n",
        "            <th style='border: 1px solid #dddddd; padding: 8px;'>Rationale</th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "\"\"\"\n",
        "\n",
        "# Add rows with alternating shading\n",
        "for i, step in enumerate(steps_summary):\n",
        "    bg_color = \"#f2f2f2\" if i % 2 == 0 else \"white\"\n",
        "    html_table += f\"\"\"\n",
        "        <tr style='border: 1px solid #dddddd; background-color: {bg_color};'>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px;'>{step[\"Original Feature\"]}</td>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px;'>{step[\"Action Taken\"]}</td>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px;'>{step[\"Rationale\"]}</td>\n",
        "        </tr>\n",
        "    \"\"\"\n",
        "\n",
        "# Add footer note inside the table as a row spanning all columns\n",
        "note_text = (\n",
        "    \"Feature Engineering completed and saved as <span style='color: green;'>FEngineered_New.csv</span> for further analysis.\"\n",
        ")\n",
        "html_table += f\"\"\"\n",
        "        <tr style='border: 1px solid #dddddd;'>\n",
        "            <td colspan=\"3\" style='border: 1px solid #dddddd; padding: 8px; background-color: #f8f8f8;'>\n",
        "                <strong>{note_text}</strong>\n",
        "            </td>\n",
        "        </tr>\n",
        "    </tbody>\n",
        "</table>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(html_table))\n",
        "df.to_csv(\"FEngineered_New.csv\", index=False)                      # Save engineered data as FEngineered_New.csv\n",
        "files.download(\"FEngineered_New.csv\")\n"
      ],
      "metadata": {
        "id": "6eviNQBXviUI"
      },
      "id": "6eviNQBXviUI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.4.4 Feature Encoding - Approach_3**"
      ],
      "metadata": {
        "id": "SyW0pJEVvo1F"
      },
      "id": "SyW0pJEVvo1F"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "                                                                                  # Read the file created in 5.4.3\n",
        "url = \"https://raw.githubusercontent.com/mohammadbadi/CrimeAnalytics_Clustering_Approach_3/refs/heads/main/Output_CSV/FEngineered_Data.csv\"\n",
        "try:\n",
        "    df_encoded = pd.read_csv(url)\n",
        "    display(HTML(\"<p style='color: green; font-size: 16px; font-weight: bold;'>Previously engineered data loaded successfully.</p>\"))\n",
        "except Exception as e:\n",
        "    display(HTML(f\"<p style='color: red; font-size: 16px; font-weight: bold;'>Error loading engineered data: {e}</p>\"))\n",
        "    exit()\n",
        "                                                                                  # Count initial columns before encoding\n",
        "initial_column_count = len(df_encoded.columns)\n",
        "                                                                                  # Build a list to store encoding steps details\n",
        "steps_summary = [\n",
        "    {\n",
        "        \"Original Feature\": \"Location_Engineered\",\n",
        "        \"Action Taken\": \"One-Hot Encoding applied to create binary features\",\n",
        "        \"Rationale\": \"Separates categories for better clustering\"\n",
        "    },\n",
        "    {\n",
        "        \"Original Feature\": \"HOOD_158\",\n",
        "        \"Action Taken\": \"Frequency encoded to 'Hood_158_Encoded'\",\n",
        "        \"Rationale\": \"Represents neighborhood distribution as normalized frequencies\"\n",
        "    },\n",
        "    {\n",
        "        \"Original Feature\": \"DIVISION\",\n",
        "        \"Action Taken\": \"Frequency encoded to 'Division_Encoded'\",\n",
        "        \"Rationale\": \"Represents division distribution as normalized frequencies\"\n",
        "    },\n",
        "    {\n",
        "        \"Original Feature\": \"OCC_MONTH\",\n",
        "        \"Action Taken\": \"Manual mapping to 'OCC_Month_Encoded'\",\n",
        "        \"Rationale\": \"Converts month names to numerical values\"\n",
        "    },\n",
        "    {\n",
        "        \"Original Feature\": \"OCC_DOW\",\n",
        "        \"Action Taken\": \"Label Encoding applied to create 'OCC_DOW_Encoded'\",\n",
        "        \"Rationale\": \"Transforms day names to numeric representations\"\n",
        "    }\n",
        "]\n",
        "                                                                                  # 1. Frequency Encoding for HOOD_158\n",
        "hood_counts = df_encoded['HOOD_158'].value_counts(normalize=True)\n",
        "df_encoded['Hood_158_Encoded'] = df_encoded['HOOD_158'].map(hood_counts)\n",
        "                                                                                  # 2. Frequency Encoding for DIVISION\n",
        "division_counts = df_encoded['DIVISION'].value_counts(normalize=True)\n",
        "df_encoded['Division_Encoded'] = df_encoded['DIVISION'].map(division_counts)\n",
        "                                                                                  # 3. One-Hot Encoding for Location_Engineered\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "location_encoded = encoder.fit_transform(df_encoded[['Location_Engineered']])\n",
        "location_encoded_df = pd.DataFrame(location_encoded,\n",
        "                                   columns=encoder.get_feature_names_out(['Location_Engineered']),\n",
        "                                   index=df_encoded.index)\n",
        "df_encoded = pd.concat([df_encoded, location_encoded_df], axis=1)\n",
        "                                                                                  # 4. Manual Mapping for OCC_MONTH\n",
        "month_mapping = {\n",
        "    'January': 1, 'February': 2, 'March': 3, 'April': 4,\n",
        "    'May': 5, 'June': 6, 'July': 7, 'August': 8,\n",
        "    'September': 9, 'October': 10, 'November': 11, 'December': 12\n",
        "}\n",
        "df_encoded['OCC_Month_Encoded'] = df_encoded['OCC_MONTH'].map(month_mapping)\n",
        "                                                                                  # 5. Label Encoding for OCC_DOW\n",
        "dow_encoder = LabelEncoder()\n",
        "df_encoded['OCC_DOW_Encoded'] = dow_encoder.fit_transform(df_encoded['OCC_DOW'])\n",
        "                                                                                  # Count final columns after encoding\n",
        "final_column_count = len(df_encoded.columns)\n",
        "                                                                                  # Build HTML Table for Feature Encoding Phase\n",
        "html_table = \"\"\"\n",
        "<table style='border-collapse: collapse; width: 100%; font-size: 18px;'>\n",
        "    <thead style='background-color: #4CAF50; color: white;'>\n",
        "        <tr>\n",
        "            <th colspan=\"3\" style=\"text-align: center; font-size: 24px; background-color: #2f4f4f; color: white;\">5.4.4 Feature Encoding Phase - Approach_3</th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <th style='border: 1px solid #dddddd; padding: 8px;'>Original Feature</th>\n",
        "            <th style='border: 1px solid #dddddd; padding: 8px;'>Action Taken</th>\n",
        "            <th style='border: 1px solid #dddddd; padding: 8px;'>Rationale</th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "\"\"\"\n",
        "                                                                                  # Add rows with alternating shading\n",
        "for i, step in enumerate(steps_summary):\n",
        "    bg_color = \"#f2f2f2\" if i % 2 == 0 else \"white\"\n",
        "    html_table += f\"\"\"\n",
        "        <tr style='border: 1px solid #dddddd; background-color: {bg_color};'>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px;'>{step[\"Original Feature\"]}</td>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px;'>{step[\"Action Taken\"]}</td>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px;'>{step[\"Rationale\"]}</td>\n",
        "        </tr>\n",
        "    \"\"\"\n",
        "                                                                                  # Add row showing columns affected with the requested formatting\n",
        "bg_color = \"#f2f2f2\" if len(steps_summary) % 2 == 0 else \"white\"\n",
        "html_table += f\"\"\"\n",
        "    <tr style='border: 1px solid #dddddd; background-color: {bg_color};'>\n",
        "        <td style='border: 1px solid #dddddd; padding: 8px;'>Columns affected in <br> <strong>5.4.4. Feature Encoding - Approach_3</strong></td>\n",
        "        <td style='border: 1px solid #dddddd; padding: 8px;'><strong>:</strong> Initial columns: <br> <strong>{initial_column_count}</strong></td>\n",
        "        <td style='border: 1px solid #dddddd; padding: 8px;'>Final Columns: <br> <strong>{final_column_count}</strong></td>\n",
        "    </tr>\n",
        "\"\"\"\n",
        "                                                                                  # Add footer note inside the table\n",
        "note_text = (\n",
        "    \"Feature Encoding completed and saved as <span style='color: green;'>Encoded_Features.csv</span> \"\n",
        "    \"for further analysis.\"\n",
        ")\n",
        "html_table += f\"\"\"\n",
        "        <tr style='border: 1px solid #dddddd;'>\n",
        "            <td colspan=\"3\" style='border: 1px solid #dddddd; padding: 8px; background-color: #f8f8f8;'><strong>{note_text}</strong></td>\n",
        "        </tr>\n",
        "    </tbody>\n",
        "</table>\n",
        "\"\"\"\n",
        "display(HTML(html_table))\n",
        "                                                                                  # Save the encoded dataset to CSV\n",
        "df_encoded.to_csv(\"FE_Encoded.csv\", index=False)\n",
        "files.download(\"FE_Encoded.csv\")\n"
      ],
      "metadata": {
        "id": "5Hj1U48pvpcO"
      },
      "id": "5Hj1U48pvpcO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.4 Data Reduction and Projection - f) Feature Engineering, g) Feature Encoding and h) Descriptive Statistics - Approach_3**\n"
      ],
      "metadata": {
        "id": "iEmi7SBxv9aW"
      },
      "id": "iEmi7SBxv9aW"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings                                                                   # Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import BoundaryNorm, LinearSegmentedColormap\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)                    # Ignore Deprecation Warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)                         # Ignore future warnings\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/mohammadbadi/CrimeAnalytics_Clustering_Approach_3/refs/heads/main/Output_CSV/FE_Encoded.csv\"    # Read the dataset from CSV file\n",
        "df_summary = pd.read_csv(url)\n",
        "\n",
        "def count_leading_trailing_spaces(column):                                        # Function to count leading/trailing spaces in strings\n",
        "    column = column.astype(str)\n",
        "    return column.str.startswith(' ').sum(), column.str.endswith(' ').sum()\n",
        "\n",
        "def prepare_summary_table(df):                                                    # Function to generate a summary table for dataset\n",
        "    unique_values = df.nunique()\n",
        "    total_values = df.count() + df.isnull().sum()\n",
        "    null_counts = df.isnull().sum()\n",
        "    nan_counts = df.isna().sum()\n",
        "    null_percentages = (null_counts / total_values) * 100\n",
        "    leading_spaces, trailing_spaces = zip(*[count_leading_trailing_spaces(df[col]) for col in df.columns])\n",
        "    summary_table = pd.DataFrame({\n",
        "        \"Column\": df.columns,\n",
        "        \"Data Type\": df.dtypes,\n",
        "        \"Total Values\": total_values,\n",
        "        \"Unique Values\": unique_values,\n",
        "        \"Null Values\": null_counts,\n",
        "        \"Null %\": null_percentages.round(1),\n",
        "        \"NaN Values\": nan_counts,\n",
        "        \"Leading Spaces\": leading_spaces,\n",
        "        \"Trailing Spaces\": trailing_spaces\n",
        "    })\n",
        "    return summary_table, null_counts, null_percentages\n",
        "\n",
        "def generate_summary_html(summary_table):\n",
        "    summary_table_html = \"\"\"\n",
        "    <style>\n",
        "        table { border-collapse: collapse; width: 100%; font-family: Arial, sans-serif; font-size: 16px; }\n",
        "        table th, table td { border: 1px solid #ddd; padding: 8px; text-align: left; }\n",
        "        table th { background-color: #4CAF50; color: white; font-size: 16px; }\n",
        "        table tr:nth-child(even) {background-color: #f2f2f2;}\n",
        "        table tr:hover {background-color: #ddd;}\n",
        "    </style>\n",
        "    <table>\n",
        "        <tr>\n",
        "            <th>Column</th>\n",
        "            <th>Data Type</th>\n",
        "            <th>Total Values</th>\n",
        "            <th>Unique Values</th>\n",
        "            <th>Null Values</th>\n",
        "            <th>Null %</th>\n",
        "            <th>NaN Values</th>\n",
        "            <th>Leading Spaces</th>\n",
        "            <th>Trailing Spaces</th>\n",
        "        </tr>\n",
        "    \"\"\"\n",
        "    for _, row in summary_table.iterrows():\n",
        "        null_color = \"red\" if row['Null Values'] > 0 else \"green\"\n",
        "        leading_color = \"red\" if row['Leading Spaces'] > 0 else \"green\"\n",
        "        trailing_color = \"red\" if row['Trailing Spaces'] > 0 else \"green\"\n",
        "        summary_table_html += f\"\"\"\n",
        "        <tr>\n",
        "            <td>{row['Column']}</td>\n",
        "            <td>{row['Data Type']}</td>\n",
        "            <td>{row['Total Values']}</td>\n",
        "            <td>{row['Unique Values']}</td>\n",
        "            <td style='color:{null_color};'>{row['Null Values']}</td>\n",
        "            <td>{row['Null %']}</td>\n",
        "            <td>{row['NaN Values']}</td>\n",
        "            <td style='color:{leading_color};'>{row['Leading Spaces']}</td>\n",
        "            <td style='color:{trailing_color};'>{row['Trailing Spaces']}</td>\n",
        "        </tr>\n",
        "        \"\"\"\n",
        "    summary_table_html += \"</table>\"\n",
        "    return summary_table_html\n",
        "\n",
        "def plot_missing_percentage(df, dataset_name):                                    # Function to plot missing data percentage\n",
        "    missing_percentage = df.isnull().mean() * 100\n",
        "    missing_percentage = missing_percentage[missing_percentage > 0]\n",
        "    if missing_percentage.empty:\n",
        "        display(HTML(f\"<p style='color: black; font-size: 16px; font-weight: bold;'>No missing data in {dataset_name}. Skipping missing percentage plot.</p>\"))\n",
        "        return \"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    missing_percentage.sort_values().plot(kind='barh', color='skyblue', edgecolor='grey')\n",
        "    plt.title(f'Missing Data Percentage - {dataset_name}', fontsize=16)\n",
        "    plt.xlabel('Missing Percentage (%)', fontsize=12)\n",
        "    plt.ylabel('Columns', fontsize=12)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    missing_plot_path = f'/content/Encoded_Data_Missing_PercentPlot.png'\n",
        "    plt.savefig(missing_plot_path)\n",
        "    plt.close()\n",
        "    display(HTML(f\"<p style='color: black; font-size: 16px; font-weight: bold;'>Missing data percentage plot saved to <span style='color: darkgreen; font-weight: bold;'>{missing_plot_path}</span>.</p>\"))\n",
        "    return missing_plot_path\n",
        "\n",
        "def display_summary_for_file(df, title):                                            # Function to display summary for dataset\n",
        "    summary_table, null_counts, null_percentages = prepare_summary_table(df)\n",
        "    summary_html = generate_summary_html(summary_table)\n",
        "    missing_plot_path = plot_missing_percentage(df, title)\n",
        "    complete_html = f\"\"\"\n",
        "    <html>\n",
        "    <head><title>Dataset Summary - {title}</title></head>\n",
        "    <body>\n",
        "        <h2 style=\"color: black; font-size: 16px; font-weight: bold;\">Summary Table for {title}</h2>\n",
        "        {summary_html}\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    summary_html_path = f'5.4.3 Summary_Encoded_Data.html'\n",
        "    with open(summary_html_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(complete_html)\n",
        "    display(HTML(f\"<p style='color: black; font-size: 16px; font-weight: bold;'>HTML summary report for {title} saved to <span style='color: darkgreen; font-weight: bold;'>{summary_html_path}</span>.</p>\"))\n",
        "    return summary_html_path, missing_plot_path\n",
        "\n",
        "html_file_path, missing_plot_path = display_summary_for_file(df_summary, \"Analysis on Encoded Data\")\n",
        "display(HTML(open(html_file_path).read()))\n",
        "\n",
        "files.download(html_file_path)                                                    # Download Summary table as HTML\n",
        "\n",
        "display(HTML(\"\"\"\n",
        "<p style=\"color: black; font-size: 16px; font-weight: bold;\">\n",
        "    Files: <span style=\"color: darkblue; font-weight: bold;\">HTML summary file</span>, <span style=\"color: darkblue; font-weight: bold;\">FEngineered_Data.csv</span> and <span style=\"color: darkblue; font-weight: bold;\">FE_Encoded.csv</span> have been <span style=\"color: darkgreen; font-weight: bold;\">downloaded</span>.\n",
        "</p>\n",
        "\"\"\"))\n"
      ],
      "metadata": {
        "id": "KbaAemn9v_iv"
      },
      "id": "KbaAemn9v_iv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.5 KMeans Feature Importance & 5.6 K-Elbow - Approach_3**"
      ],
      "metadata": {
        "id": "7EqpQxYawEyd"
      },
      "id": "7EqpQxYawEyd"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings                                                                   # Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)                    # Ignore Deprecation Warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)                         # Ignore Future Warnings\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/mohammadbadi/CrimeAnalytics_Clustering_Approach_3/refs/heads/main/Output_CSV/FE_Encoded.csv\"  # Read the dataset from CSV file\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "continuous_features = ['OCC_YEAR', 'OCC_DOY', 'OCC_HOUR', 'LONG_WGS84', 'LAT_WGS84']  # Define Continuous Features\n",
        "encoded_features = ['OCC_Month_Encoded', 'OCC_DOW_Encoded', 'Hood_158_Encoded',   # Define Encoded Features\n",
        "                    'Division_Encoded', 'Location_Engineered_Other',\n",
        "                    'Location_Engineered_Public', 'Location_Engineered_Residential']\n",
        "features = continuous_features + encoded_features\n",
        "                                                                                  # Build preprocessor and pipeline for KMeans Clustering\n",
        "scaler = StandardScaler()                                                         # Standardization\n",
        "preprocessor = ColumnTransformer([('num', scaler, features)])\n",
        "pipeline = Pipeline([                                                             # KMeans Clustering Pipeline (n_clusters=4 based on K-Elbow)\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('kmeans', KMeans(n_clusters=4, random_state=42, n_init=10))\n",
        "])\n",
        "pipeline.fit(df)                                                                  # Fit the Pipeline\n",
        "df_processed = pipeline.transform(df)                                             # Transform the Data\n",
        "\n",
        "kmeans = pipeline.named_steps['kmeans']                                           # Extract KMeans Model, assign cluster labels, compute centroid variance\n",
        "df['Cluster'] = kmeans.labels_\n",
        "centroids = kmeans.cluster_centers_\n",
        "centroid_variance = np.var(centroids, axis=0)\n",
        "importance_df = pd.DataFrame({'Feature': features, 'Centroid Variance': centroid_variance})\\\n",
        "                  .sort_values(by='Centroid Variance', ascending=False)\n",
        "\n",
        "sil_score = silhouette_score(df_processed, kmeans.labels_)                        # Compute Silhouette Score, Davies-Bouldin Index, Calinski-Harabasz Index, and Inertia\n",
        "db_index = davies_bouldin_score(df_processed, kmeans.labels_)\n",
        "ch_index = calinski_harabasz_score(df_processed, kmeans.labels_)\n",
        "inertia_value = kmeans.inertia_\n",
        "                                                                                  # Plot: K-Elbow Method\n",
        "inertia_values = []                                                               # Compute inertia for K values from 2 to 9\n",
        "K_range = range(2, 10)\n",
        "for k in K_range:\n",
        "    kmeans_test = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans_test.fit(df_processed)\n",
        "    inertia_values.append(kmeans_test.inertia_)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(K_range, inertia_values, marker='o', linestyle='-')\n",
        "plt.xlabel(\"Number of Clusters (K)\")\n",
        "plt.ylabel(\"Inertia (Within-Cluster Sum of Squares)\")\n",
        "plt.title(\"Elbow Method for Optimal K - Approach_3\")\n",
        "plt.grid(True)\n",
        "plt.savefig(\"5.5 K-Elbow.png\", dpi=300, bbox_inches='tight')                      # Save Elbow plot\n",
        "plt.show()\n",
        "files.download(\"5.5 K-Elbow.png\")                                                 # Download the Elbow plot\n",
        "display(HTML(\"<br><br>\"))\n",
        "                                                                                  # Text Output: Inertia Values\n",
        "inertia_html = \"\"\"\n",
        "<p style=\"color: darkblue; font-size: 18px; font-weight: bold;\">\n",
        "    Inertia values for different K:\n",
        "\"\"\"\n",
        "for k, inertia in zip(K_range, inertia_values):\n",
        "    inertia_html += f\"<br>For K = <span style='color: blue;'>{k}</span>: <span style='color: blue;'>{inertia:.2f}</span>\"\n",
        "inertia_html += \"</p>\"\n",
        "display(HTML(inertia_html))\n",
        "display(HTML(\"<br><br>\"))\n",
        "                                                                                  #  Plot: K-Means Feature Importance\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=importance_df, x='Centroid Variance', y='Feature', palette='viridis')\n",
        "plt.xlabel(\"Centroid Variance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Feature Importance in KMeans Clustering - Approach_3\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.savefig(\"5.6 K-Means Feature Importance.png\", dpi=300, bbox_inches='tight')   # Save Feature Importance plot\n",
        "plt.show()\n",
        "files.download(\"5.6 K-Means Feature Importance.png\")                              # Download Feature Importance plot\n",
        "display(HTML(\"<br><br>\"))\n",
        "                                                                                  # Plot: Number of Points in Each Cluster\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=df, x='Cluster', hue='Cluster', palette=\"Set2\", dodge=False)\n",
        "plt.title(\"Number of Points in Each Cluster - Approach_3\")\n",
        "plt.xlabel(\"Cluster Label\")\n",
        "plt.ylabel(\"Number of Data Points\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.savefig(\"5.6 K-Means Cluster Distribution.png\", dpi=300, bbox_inches='tight') # Save Cluster Distribution plot\n",
        "plt.show()\n",
        "files.download(\"5.6 K-Means Cluster Distribution.png\")                            # Download Cluster Distribution plot\n",
        "display(HTML(\"<br><br>\"))\n",
        "                                                                                  # Text Output: Cluster Distribution\n",
        "cluster_counts = df['Cluster'].value_counts().sort_index()\n",
        "cluster_html = \"\"\"\n",
        "<p style=\"color: darkblue; font-size: 18px; font-weight: bold;\">\n",
        "    Number of data points in each cluster:\n",
        "\"\"\"\n",
        "for cluster, count in cluster_counts.items():\n",
        "    cluster_html += f\"<br><span style='color: blue;'>Cluster {cluster}</span>: <span style='color: green;'>{count}</span> data points\"\n",
        "cluster_html += \"</p>\"\n",
        "display(HTML(cluster_html))\n",
        "display(HTML(\"<br><br>\"))\n",
        "                                                                                  # Build HTML Table for K-Means Clustering Analysis\n",
        "html_table = f\"\"\"\n",
        "<table style='border-collapse: collapse; font-size: 18px; width: 100%; max-width: 900px; table-layout: fixed;'>\n",
        "    <thead>\n",
        "        <tr style='background-color: #2f4f4f; color: white;'>\n",
        "            <th colspan=\"3\" style=\"text-align: center; font-size: 24px; padding: 8px;\">\n",
        "                KMEANS Clustering Analysis - Approach_3\n",
        "            </th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td colspan=\"3\" style=\"border: 1px solid #dddddd; padding: 8px; text-align: center; white-space: normal; word-wrap: break-word;\">\n",
        "                Based on the Optimal K value from the K-Elbow method, KMeans was performed with 4 clusters (n=4), which produced the following clustering statistics:\n",
        "                <strong>Silhouette Score:</strong> {sil_score:.2f},\n",
        "                <strong>Davies-Bouldin Index:</strong> {db_index:.2f},\n",
        "                <strong>Calinski-Harabasz Index:</strong> {ch_index:.2f}, and\n",
        "                <strong>Inertia:</strong> {inertia_value:.2f}.\n",
        "                Centroid Variance represents the variance of the cluster centroids for each feature, reflecting its contribution to the clustering structure.\n",
        "            </td>\n",
        "        </tr>\n",
        "        <tr style='background-color: #4CAF50; color: white;'>\n",
        "            <th style='border: 1px solid #dddddd; padding: 8px;'>S/N</th>\n",
        "            <th style='border: 1px solid #dddddd; padding: 8px;'>Feature</th>\n",
        "            <th style='border: 1px solid #dddddd; padding: 8px;'>Centroid Variance</th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "\"\"\"\n",
        "for idx, row in enumerate(importance_df.itertuples(), start=1):                   # Add a row for each feature's centroid variance\n",
        "    feature_val = row.Feature\n",
        "    variance_val = f\"{row[2]:.4f}\"                                                # row[2] is the 'Centroid Variance'\n",
        "    html_table += f\"\"\"\n",
        "        <tr style='border: 1px solid #dddddd;'>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px; text-align: center;'>{idx}</td>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px;'>{feature_val}</td>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px; text-align: right;'>{variance_val}</td>\n",
        "        </tr>\n",
        "    \"\"\"\n",
        "html_table += \"\"\"\n",
        "    </tbody>\n",
        "</table>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(html_table))                                                         # Display the HTML table\n",
        "display(HTML(\"<br><br>\"))\n",
        "                                                                                  # Save the HTML table as file \"5.6 K-Means Feature Importance.html\" and download it\n",
        "html_table_path = \"5.6 K-Means Feature Importance.html\"\n",
        "with open(html_table_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(html_table)\n",
        "files.download(html_table_path)                                                   # Download the HTML table file\n",
        "df.to_csv(\"FE_Encoded_with_Clusters.csv\", index=False)                            # Save the updated dataset with cluster labels\n",
        "files.download(\"FE_Encoded_with_Clusters.csv\")                                    # Download the updated dataset with cluster labels\n",
        "                                                                                  # Rationale Explanation for Choosing n=4\n",
        "if len(inertia_values) >= 5:\n",
        "    inertia_reduction_k3_k4 = ((inertia_values[1] - inertia_values[2]) / inertia_values[1]) * 100\n",
        "    inertia_reduction_k4_k5 = ((inertia_values[2] - inertia_values[3]) / inertia_values[2]) * 100\n",
        "else:\n",
        "    inertia_reduction_k3_k4 = inertia_reduction_k4_k5 = 0\n",
        "\n",
        "explanation = f\"\"\"\n",
        "<p style=\"color: black; font-size: 18px; font-weight: bold;\">\n",
        "    The K-Elbow method shows that the inertia value decreases notably as K increases from <span style=\"color: blue;\">2</span> to <span style=\"color: blue;\">4</span>. For example, when K increases from <span style=\"color: blue;\">2</span> to <span style=\"color: blue;\">3</span>, inertia decreases from <span style=\"color: blue;\">{inertia_values[0]:.2f}</span> to <span style=\"color: blue;\">{inertia_values[1]:.2f}</span>, and from K = <span style=\"color: blue;\">3</span> to K = <span style=\"color: blue;\">4</span> it decreases to <span style=\"color: blue;\">{inertia_values[2]:.2f}</span> (a reduction of <span style=\"color: blue;\">{inertia_reduction_k3_k4:.1f}%</span>). Beyond K = <span style=\"color: blue;\">4</span>, the reduction in inertia becomes less dramatic (e.g. a further reduction of <span style=\"color: blue;\">{inertia_reduction_k4_k5:.1f}%</span> from K = <span style=\"color: blue;\">4</span> to K = <span style=\"color: blue;\">5</span>).\n",
        "</p>\n",
        "<p style=\"color: black; font-size: 18px; font-weight: bold;\">\n",
        "    The cluster distribution is also reasonable:\n",
        "    <br><span style=\"color: blue;\">Cluster 0</span>: <span style=\"color: green;\">{cluster_counts.get(0, 0)}</span> data points\n",
        "    <br><span style=\"color: blue;\">Cluster 1</span>: <span style=\"color: green;\">{cluster_counts.get(1, 0)}</span> data points\n",
        "    <br><span style=\"color: blue;\">Cluster 2</span>: <span style=\"color: green;\">{cluster_counts.get(2, 0)}</span> data points\n",
        "    <br><span style=\"color: blue;\">Cluster 3</span>: <span style=\"color: green;\">{cluster_counts.get(3, 0)}</span> data points\n",
        "</p>\n",
        "<p style=\"color: black; font-size: 18px; font-weight: bold;\">\n",
        "    Based on the significant drop in inertia up to K = <span style=\"color: blue;\">4</span> and a balanced cluster distribution, using <span style=\"color: blue;\">4</span> clusters (n=4) for KMeans clustering is a good choice.\n",
        "</p>\n",
        "\"\"\"\n",
        "display(HTML(explanation))"
      ],
      "metadata": {
        "id": "s-uyVq1dwGlf"
      },
      "id": "s-uyVq1dwGlf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.7 Clustering Model Training - Approach 1**"
      ],
      "metadata": {
        "id": "3EG5MZL8wKtV"
      },
      "id": "3EG5MZL8wKtV"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings                                                                   # Import necessary libraries\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)                    # Ignore Deprecation Warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)                         # Ignore future warnings\n",
        "\n",
        "features = [                                                                      # Define feature list\n",
        "    'OCC_YEAR', 'OCC_DOY', 'OCC_HOUR', 'LONG_WGS84', 'LAT_WGS84',\n",
        "    'OCC_Month_Encoded', 'OCC_DOW_Encoded', 'Hood_158_Encoded', 'Division_Encoded',\n",
        "    'Location_Engineered_Other', 'Location_Engineered_Public', 'Location_Engineered_Residential'\n",
        "]\n",
        "\n",
        "all_combinations = [list(combo) for r in range(4, 8) for combo in itertools.combinations(features, r)]  # Generate feature combinations (sizes 4, 5, 6 and 7)\n",
        "\n",
        "formatted_combinations_df = pd.DataFrame(all_combinations)                        # Save feature combinations to CSV\n",
        "formatted_combinations_df.to_csv('Feature_Combo_Current.csv', index=False)\n",
        "display(HTML(\"<p style='color: green; font-size:16px;'><b>Feature sets saved to 'Feature_Combo_Current.csv'</b></p>\"))\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/mohammadbadi/CrimeAnalytics_Clustering_Approach_3/refs/heads/main/Output_CSV/FE_Encoded.csv\"  # Read the dataset from CSV file\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "sample_data = data.sample(frac=0.1, random_state=42)                              # Sample 10% of the data for clustering\n",
        "results = {}                                                                      # Track results - to store clustering metrics for each feature set\n",
        "set_counters = {}                                                                 # To track counts per number of features\n",
        "total_models = 0\n",
        "\n",
        "def color_silhouette(value):                                                      # HTML helper functions for color coding Silhouette Score\n",
        "    \"\"\"Color code Silhouette Score (higher is better)\"\"\"\n",
        "    if value >= 0.6:\n",
        "        color = \"#32cd32\"                                                         # green\n",
        "    elif value >= 0.3:\n",
        "        color = \"#ffcc00\"                                                         # yellow\n",
        "    else:\n",
        "        color = \"#ff6347\"                                                         # red\n",
        "    return f\"<span style='color: {color}; font-weight: bold;'>{value:.2f}</span>\"\n",
        "\n",
        "def color_dbi(value):                                                             # HTML helper functions for color coding DB index\n",
        "    \"\"\"Color code Davies-Bouldin Index (lower is better)\"\"\"\n",
        "    if value <= 1.0:\n",
        "        color = \"#32cd32\"                                                         # green\n",
        "    elif value <= 1.5:\n",
        "        color = \"#ffcc00\"                                                         # yellow\n",
        "    else:\n",
        "        color = \"#ff6347\"                                                         # red\n",
        "    return f\"<span style='color: {color}; font-weight: bold;'>{value:.2f}</span>\"\n",
        "\n",
        "def color_accuracy(value):                                                        # HTML helper functions for color coding Accuracy Percentage Score\n",
        "    \"\"\"Color code prediction accuracy: below 50% red, 50.1%+ green\"\"\"\n",
        "    if value < 50.1:\n",
        "        color = \"#ff6347\"                                                         # red\n",
        "    else:\n",
        "        color = \"#32cd32\"                                                         # green\n",
        "    return f\"<span style='color: {color}; font-weight: bold;'>{value:.2f}%</span>\"\n",
        "\n",
        "for i, feature_set in enumerate(all_combinations):                                # Iterate through feature sets\n",
        "    total_models += 1\n",
        "    valid_features = [f for f in feature_set if f in sample_data.columns]         # Select valid features (ensuring they exist in the dataset)\n",
        "    if len(valid_features) != len(feature_set):\n",
        "        print(f\"Warning: Some features in {feature_set} are missing. Using only: {valid_features}\")\n",
        "    data_for_clustering = sample_data[valid_features]\n",
        "    numerical_cols = data_for_clustering.select_dtypes(include=['int64', 'float64']).columns.tolist()     # Standardize numerical columns\n",
        "    scaler = StandardScaler()\n",
        "    data_scaled = pd.DataFrame(scaler.fit_transform(data_for_clustering[numerical_cols]), columns=numerical_cols)\n",
        "    categorical_cols = [col for col in valid_features if col not in numerical_cols]       # Append any categorical columns\n",
        "    if categorical_cols:\n",
        "        data_scaled = pd.concat([data_scaled, data_for_clustering[categorical_cols]], axis=1)\n",
        "    kmeans = KMeans(n_clusters=4, random_state=42)                                # Perform KMeans clustering\n",
        "    kmeans_labels = kmeans.fit_predict(data_scaled)\n",
        "    dbscan = DBSCAN(eps=0.5, min_samples=5)                                       # DBSCAN Clustering\n",
        "    dbscan_labels = dbscan.fit_predict(data_scaled)\n",
        "\n",
        "    silhouette_kmeans = silhouette_score(data_scaled, kmeans_labels)              # Calculate KMeans metrics\n",
        "    dbi_kmeans = davies_bouldin_score(data_scaled, kmeans_labels)\n",
        "    ch_kmeans = calinski_harabasz_score(data_scaled, kmeans_labels)\n",
        "    kmeans_accuracy = max(0, silhouette_kmeans) * 100\n",
        "\n",
        "    silhouette_dbscan = -1 if len(set(dbscan_labels)) <= 1 else silhouette_score(data_scaled, dbscan_labels)     # Calculate DBSCAN metrics\n",
        "    dbi_dbscan = -1 if len(set(dbscan_labels)) <= 1 else davies_bouldin_score(data_scaled, dbscan_labels)\n",
        "    dbscan_accuracy = max(0, silhouette_dbscan) * 100\n",
        "\n",
        "    num_features = len(valid_features)                                            # Update set_counters\n",
        "    if num_features not in set_counters:\n",
        "        set_counters[num_features] = 0\n",
        "    set_counters[num_features] += 1\n",
        "    set_number = set_counters[num_features]\n",
        "\n",
        "    formatted_features = [f\"'{feature}'\" for feature in valid_features]           # Format features string\n",
        "    feature_names_string = f\"[{', '.join(formatted_features)}]\"\n",
        "\n",
        "    results[tuple(valid_features)] = {                                            # Store results in dictionary\n",
        "        'Feature Set': f\"{num_features}_Set_{set_number}\",\n",
        "        'Number of Features': num_features,\n",
        "        'KMeans Silhouette Score': silhouette_kmeans,\n",
        "        'KMeans Davies-Bouldin Index': dbi_kmeans,\n",
        "        'KMeans Calinski-Harabasz Score': ch_kmeans,\n",
        "        'KMeans Prediction Accuracy': kmeans_accuracy,\n",
        "        'DBSCAN Silhouette Score': silhouette_dbscan,\n",
        "        'DBSCAN Davies-Bouldin Index': dbi_dbscan,\n",
        "        'DBSCAN Prediction Accuracy': dbscan_accuracy,\n",
        "        'Feature_Names_String': feature_names_string\n",
        "    }\n",
        "\n",
        "                                                                                  # Create HTML table output for current result\n",
        "    results_html = f\"\"\"\n",
        "    <div style=\"margin: 10px auto; width: 80%; border: 2px solid #ddd; padding: 10px; background-color: #f9f9f9;\">\n",
        "      <table style=\"width: 100%; border-collapse: collapse; text-align: center;\">\n",
        "        <tr>\n",
        "          <th colspan=\"3\" style=\"font-size: 14px; padding: 10px; text-align: left;\">Result of Feature Set {num_features}_Set_{set_number}</th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "          <th colspan=\"3\" style=\"font-size: 13px; padding: 5px; text-align: left;\">Features : {feature_names_string}</th>\n",
        "        </tr>\n",
        "        <tr style=\"background-color: #e0e0e0; font-weight: bold;\">\n",
        "          <td style=\"padding: 8px; border: 1px solid #ddd;\">Statistic</td>\n",
        "          <td style=\"padding: 8px; border: 1px solid #ddd;\">KMeans</td>\n",
        "          <td style=\"padding: 8px; border: 1px solid #ddd;\">DBSCAN</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "          <td style=\"padding: 8px; border: 1px solid #ddd;\">Silhouette Score</td>\n",
        "          <td style=\"padding: 8px; border: 1px solid #ddd;\">{color_silhouette(silhouette_kmeans)}</td>\n",
        "          <td style=\"padding: 8px; border: 1px solid #ddd;\">{\"N/A\" if silhouette_dbscan == -1 else color_silhouette(silhouette_dbscan)}</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "          <td style=\"padding: 8px; border: 1px solid #ddd;\">Davies-Bouldin Index</td>\n",
        "          <td style=\"padding: 8px; border: 1px solid #ddd;\">{color_dbi(dbi_kmeans)}</td>\n",
        "          <td style=\"padding: 8px; border: 1px solid #ddd;\">{\"N/A\" if dbi_dbscan == -1 else color_dbi(dbi_dbscan)}</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "          <td style=\"padding: 8px; border: 1px solid #ddd;\">Calinski-Harabasz Score</td>\n",
        "          <td style=\"padding: 8px; border: 1px solid #ddd;\">{round(ch_kmeans,2)}</td>\n",
        "          <td style=\"padding: 8px; border: 1px solid #ddd;\">-</td>\n",
        "        </tr>\n",
        "        <tr style=\"font-weight: bold;\">\n",
        "          <td style=\"padding: 8px; border: 1px solid #ddd;\">Prediction Accuracy</td>\n",
        "          <td style=\"padding: 8px; border: 1px solid #ddd;\">{color_accuracy(kmeans_accuracy)}</td>\n",
        "          <td style=\"padding: 8px; border: 1px solid #ddd;\">{color_accuracy(dbscan_accuracy)}</td>\n",
        "        </tr>\n",
        "      </table>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(results_html))\n",
        "    time.sleep(0.1)\n",
        "\n",
        "results_df = pd.DataFrame(results.values())                                         # Convert results to DataFrame\n",
        "results_df.to_csv('Feature_Combo_Current_Results.csv', index=False)                 # Save the dataframe as CSV File\n",
        "files.download('Feature_Combo_Current_Results.csv')                                 # Download the CSV file\n",
        "                                                                                    # Display formatted message for saved file\n",
        "display(HTML(\"\"\"\n",
        "    <p style=\"color: darkblue; font-size: 18px; font-weight: bold;\">\n",
        "         Results have been saved as <span style=\"color: green;\">Feature_Combo_Current_Results.csv</span>.\n",
        "    </p>\n",
        "\"\"\"))\n",
        "display(HTML(f\"<h2 style='color: navy; font-size:18px;'><b>Total K-Means & DBSCAN Clustering Models Trained: {total_models}</b></h2>\")) # Display total models trained\n",
        "print(\"\\n\\n\")\n",
        "                                                                                    # Final summary display\n",
        "display(HTML(f\"\"\"\n",
        "<div style=\"font-family: Arial, sans-serif; font-size: 18px; padding: 15px; border-radius: 10px;\n",
        "             background: #282c34; color: #61dafb; text-align: center; width: 60%; margin: 20px auto;\n",
        "             box-shadow: 2px 2px 10px rgba(0,0,0,0.2);\">\n",
        "    <strong>Total K-Means & DBSCAN Clustering models trained:</strong>\n",
        "    <span style=\"color: #ffcc00; font-size: 22px;\">{total_models}</span>\n",
        "</div>\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "YNmnH0BnwM8j"
      },
      "id": "YNmnH0BnwM8j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary Table of Clustering Models - Approach_3**"
      ],
      "metadata": {
        "id": "M6d4_DwHwOvL"
      },
      "id": "M6d4_DwHwOvL"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings                                                                   # Import necessary libraries\n",
        "import pandas as pd\n",
        "import asyncio\n",
        "!pip install dataframe_image -qqq\n",
        "import dataframe_image as dfi\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)                    # Ignore Deprecation Warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)                         # Ignore future warnings\n",
        "\n",
        "# Read the CSV file\n",
        "url = \"https://raw.githubusercontent.com/mohammadbadi/CrimeAnalytics_Clustering_Approach_3/refs/heads/main/Output_CSV/Feature_Combo_Current_Results.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Define the metrics to evaluate\n",
        "metrics = [\n",
        "    'KMeans Silhouette Score',\n",
        "    'KMeans Calinski-Harabasz Score',\n",
        "    'DBSCAN Silhouette Score',\n",
        "    'KMeans Davies-Bouldin Index',\n",
        "    'DBSCAN Davies-Bouldin Index'\n",
        "]\n",
        "\n",
        "# Initialize dictionaries for results and summary counts\n",
        "top_results = {}\n",
        "feature_set_summary = {}\n",
        "\n",
        "# Iterate through the top 100 rows for each metric and store occurrences\n",
        "for metric in metrics:\n",
        "    if metric in data.columns:\n",
        "        # For Davies-Bouldin Index, lower values are better; otherwise higher is better\n",
        "        if 'Davies-Bouldin' in metric:\n",
        "            top_rows = data.nsmallest(100, metric)\n",
        "        else:\n",
        "            top_rows = data.nlargest(100, metric)\n",
        "        top_results[metric] = top_rows\n",
        "        for _, row in top_rows.iterrows():\n",
        "            feature_set = row['Feature Set']\n",
        "            if feature_set not in feature_set_summary:\n",
        "                feature_set_summary[feature_set] = {\n",
        "                    'Count': 0,\n",
        "                    'Found In': []\n",
        "                }\n",
        "            feature_set_summary[feature_set]['Count'] += 1\n",
        "            feature_set_summary[feature_set]['Found In'].append(metric)\n",
        "\n",
        "# Create a summary DataFrame for the most repeated feature sets\n",
        "summary_df = pd.DataFrame.from_dict(feature_set_summary, orient='index')\n",
        "summary_df.reset_index(inplace=True)\n",
        "summary_df.columns = ['Feature Set', 'Count', 'Found In']\n",
        "summary_df.sort_values(by='Count', ascending=False, inplace=True)\n",
        "\n",
        "# Prepare the final metrics DataFrame with an extra 'Features' column\n",
        "final_metrics_df = pd.DataFrame(columns=['Feature Set', 'Features', 'Count', *metrics])\n",
        "for index, row in summary_df.iterrows():\n",
        "    feature_set_name = row['Feature Set']\n",
        "    metrics_row = data[data['Feature Set'] == feature_set_name]\n",
        "    if not metrics_row.empty:\n",
        "        new_row = {\n",
        "            'Feature Set': feature_set_name,\n",
        "            'Features': metrics_row['Feature_Names_String'].values[0],  # Get value from Feature_Names_String column\n",
        "            'Count': row['Count'],\n",
        "            **{metric: metrics_row[metric].values[0] for metric in metrics}\n",
        "        }\n",
        "        final_metrics_df = pd.concat([final_metrics_df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "final_metrics_df.sort_values(by='Count', ascending=False, inplace=True)\n",
        "\n",
        "# Define a function to highlight the top 5 unique values for a given metric\n",
        "def highlight_best_top5(s, metric):\n",
        "    # For Davies-Bouldin, lower is better; for others, higher is better\n",
        "    if 'Davies-Bouldin' in metric:\n",
        "        sorted_values = s.sort_values(ascending=True)\n",
        "    else:\n",
        "        sorted_values = s.sort_values(ascending=False)\n",
        "    top5_values = sorted_values.unique()[:5]\n",
        "    return ['background-color: lightgreen' if x in top5_values else '' for x in s]\n",
        "\n",
        "# Style the full metrics summary table Approach_3\n",
        "styled_table = final_metrics_df.style.apply(highlight_best_top5, metric='KMeans Silhouette Score', subset=['KMeans Silhouette Score']) \\\n",
        "                                      .apply(highlight_best_top5, metric='KMeans Calinski-Harabasz Score', subset=['KMeans Calinski-Harabasz Score']) \\\n",
        "                                      .apply(highlight_best_top5, metric='KMeans Davies-Bouldin Index', subset=['KMeans Davies-Bouldin Index']) \\\n",
        "                                      .apply(highlight_best_top5, metric='DBSCAN Silhouette Score', subset=['DBSCAN Silhouette Score']) \\\n",
        "                                      .apply(highlight_best_top5, metric='DBSCAN Davies-Bouldin Index', subset=['DBSCAN Davies-Bouldin Index']) \\\n",
        "                                      .format({metric: '{:.2f}' for metric in metrics}) \\\n",
        "                                      .set_table_styles([\n",
        "                                          {'selector': 'th', 'props': [('background-color', '#4CAF50'),\n",
        "                                                                       ('color', 'white'),\n",
        "                                                                       ('font-weight', 'bold'),\n",
        "                                                                       ('text-align', 'center')]},\n",
        "                                          {'selector': 'td', 'props': [('padding', '10px'),\n",
        "                                                                       ('text-align', 'center')]},\n",
        "                                          {'selector': '.row:hover', 'props': [('background-color', '#f1f1f1')]}\n",
        "                                      ]) \\\n",
        "                                      .set_properties(**{'border': '1px solid black'}) \\\n",
        "                                      .set_caption(\"<h3 style='color: navy; text-align: center;'>📊 Metrics Summary Table Approach_3</h3>\")\n",
        "\n",
        "# Save the styled table to Excel and PNG files\n",
        "styled_table.data.to_excel('5.8 metrics_summary_table_Approach_3.xlsx', index=False)\n",
        "dfi.export(styled_table.data, '5.8 metrics_summary_table_Approach_3.png', table_conversion='matplotlib', max_rows=-1)\n",
        "files.download('5.8 metrics_summary_table_Approach_3.png')\n",
        "files.download('5.8 metrics_summary_table_Approach_3.xlsx')\n",
        "\n",
        "# Create an extra table displaying the top 5 models (sorted by Count)\n",
        "top5_df = final_metrics_df.head(5).copy()\n",
        "styled_top5 = top5_df.style.apply(lambda s: highlight_best_top5(s, 'KMeans Silhouette Score'), subset=['KMeans Silhouette Score']) \\\n",
        "                              .apply(lambda s: highlight_best_top5(s, 'KMeans Calinski-Harabasz Score'), subset=['KMeans Calinski-Harabasz Score']) \\\n",
        "                              .apply(lambda s: highlight_best_top5(s, 'KMeans Davies-Bouldin Index'), subset=['KMeans Davies-Bouldin Index']) \\\n",
        "                              .apply(lambda s: highlight_best_top5(s, 'DBSCAN Silhouette Score'), subset=['DBSCAN Silhouette Score']) \\\n",
        "                              .apply(lambda s: highlight_best_top5(s, 'DBSCAN Davies-Bouldin Index'), subset=['DBSCAN Davies-Bouldin Index']) \\\n",
        "                              .format({metric: '{:.2f}' for metric in metrics}) \\\n",
        "                              .set_table_styles([\n",
        "                                  {'selector': 'th', 'props': [('background-color', '#4CAF50'),\n",
        "                                                               ('color', 'white'),\n",
        "                                                               ('font-weight', 'bold'),\n",
        "                                                               ('text-align', 'center'),\n",
        "                                                               ('border', '1px solid black')]},\n",
        "                                  {'selector': 'td', 'props': [('padding', '10px'),\n",
        "                                                               ('text-align', 'center'),\n",
        "                                                               ('border', '1px solid black')]},\n",
        "                                  {'selector': 'table', 'props': [('border-collapse', 'collapse')]}\n",
        "                              ]) \\\n",
        "                              .set_properties(**{'border': '1px solid black'}) \\\n",
        "                              .set_caption(\"<h3 style='color: navy; text-align: center;'>Top 5 Models in Approach_3</h3>\")\n",
        "\n",
        "display(styled_top5)\n",
        "display(styled_table)\n",
        "\n",
        "# Export the top models table as a PNG file and download it\n",
        "dfi.export(styled_top5.data, 'Top 5 models in Approach_3.png', table_conversion='matplotlib', max_rows=-1)\n",
        "files.download('Top 5 models in Approach_3.png')\n",
        "\n",
        "display(HTML(\"\"\"\n",
        "    <p style=\"color: darkblue; font-size: 18px; font-weight: bold;\">\n",
        "         Metrics summary table Appraoch_3 has been saved to <span style=\"color: green;\">'5.8 metrics_summary_table_Approach_3.xlsx'</span>\n",
        "         and <span style=\"color: green;\">'5.8 metrics_summary_table_Approach_3.png'</span>.\n",
        "    </p>\n",
        "\"\"\"))\n"
      ],
      "metadata": {
        "id": "zKN4WRIdwSv9"
      },
      "id": "zKN4WRIdwSv9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.9 Best Model Training - Approach 3**"
      ],
      "metadata": {
        "id": "oWyjY0VOhVpD"
      },
      "id": "oWyjY0VOhVpD"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings                                                                   # Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast                                                                        # For safely evaluating strings\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)                    # Ignore Deprecation Warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)                         # Ignore future warnings\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/mohammadbadi/CrimeAnalytics_Clustering_Approach_3/refs/heads/main/Output_CSV/FE_Encoded.csv\"  # Read the dataset from CSV file\n",
        "url1 = \"https://raw.githubusercontent.com/mohammadbadi/CrimeAnalytics_Clustering_Approach_3/refs/heads/main/Output_CSV/Feature_Combo_Current_Results.csv\"  # Load feature combinations\n",
        "original_data = pd.read_csv(url)\n",
        "feature_combos = pd.read_csv(url1)\n",
        "\n",
        "                                                                                  # Debugging: Check input files for missing values\n",
        "html_output = \"\"\"\n",
        "<p style=\"color: black; font-size: 16px; font-weight: bold;\">\n",
        "    Checking input files...<br>\n",
        "    Missing values in original_data: <span style=\"color: green;\">{orig_missing}</span><br>\n",
        "    Missing values in feature_combos: <span style=\"color: green;\">{feat_missing}</span>\n",
        "</p>\n",
        "\"\"\"\n",
        "display(HTML(html_output.format(orig_missing=original_data.isnull().sum().sum(), feat_missing=feature_combos.isnull().sum().sum())))\n",
        "\n",
        "original_data['_id'] = original_data.index                                        # Store _id before clustering\n",
        "sample_data = original_data.copy()\n",
        "#sample_data = original_data.sample(frac=0.1, random_state=42)                     # Use 10% of the data (for now)\n",
        "\n",
        "# Define feature sets\n",
        "set_names = ['4_Set_165','4_Set_369', '4_Set_490', '4_Set_494', '4_Set_495']      # Define the set names to match\n",
        "feature_sets = []                                                                 # Initialize an empty list to store feature sets\n",
        "for set_name in set_names:                                                        # Iterate through the set names\n",
        "    matched_features = feature_combos[feature_combos['Feature Set'] == set_name]['Feature_Names_String']  # Extract corresponding feature sets\n",
        "    if not matched_features.empty:\n",
        "        features_list = ast.literal_eval(matched_features.values[0])              # Convert string to list\n",
        "        feature_sets.append(features_list)\n",
        "    else:\n",
        "        feature_sets.append([])                                                   # Handle missing feature sets\n",
        "\n",
        "                                                                                  # Debugging: Check feature set validity\n",
        "debug_feature = \"<p style=\\\"color: black; font-size: 16px; font-weight: bold;\\\">Checking feature set validity.<br>\"\n",
        "for i, features in enumerate(feature_sets, start=1):\n",
        "    valid_features = [f for f in features if f in sample_data.columns]\n",
        "    debug_feature += f\"<span style=\\\"color: darkblue;\\\">{set_names[i-1]}</span>: <span style=\\\"color: green;\\\">{len(valid_features)}</span> valid features out of <span style=\\\"color: green;\\\">{len(features)}</span><br>\"\n",
        "    if len(valid_features) == 0:\n",
        "        debug_feature += f\"Warning: Feature set <span style=\\\"color: darkblue;\\\">{set_names[i-1]}</span> has no valid features!<br>\"\n",
        "debug_feature += \"</p>\"\n",
        "display(HTML(debug_feature))\n",
        "\n",
        "                                                                                  # Debugging: Standardization Check for each feature set\n",
        "debug_standard = \"<p style=\\\"color: black; font-size: 16px; font-weight: bold;\\\">Standardization Checks:<br>\"\n",
        "for i, features in enumerate(feature_sets, start=1):\n",
        "    valid_features = [f for f in features if f in sample_data.columns]\n",
        "    if valid_features:\n",
        "        data_for_clustering = sample_data[valid_features].copy()\n",
        "        numerical_cols = data_for_clustering.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "        if len(numerical_cols) == 0:\n",
        "            debug_standard += f\" Warning: Feature set <span style=\\\"color: darkblue;\\\">{set_names[i-1]}</span> has NO numerical columns for clustering!<br>\"\n",
        "        else:\n",
        "            scaler = StandardScaler()\n",
        "            try:\n",
        "                scaled_data = scaler.fit_transform(data_for_clustering[numerical_cols])\n",
        "                debug_standard += f\" Scaling successful for <span style=\\\"color: darkblue;\\\">{set_names[i-1]}</span> (<span style=\\\"color: green;\\\">{len(numerical_cols)}</span> numerical features).<br>\"\n",
        "            except Exception as e:\n",
        "                debug_standard += f\" Error scaling <span style=\\\"color: darkblue;\\\">{set_names[i-1]}</span>: {e}<br>\"\n",
        "debug_standard += \"</p>\"\n",
        "display(HTML(debug_standard))\n",
        "\n",
        "                                                                                  # Debugging: Check _id mapping\n",
        "sample_ids = sample_data['_id'].values                                            # Store _id for mapping\n",
        "duplicated_ids = sample_data['_id'].duplicated().sum()\n",
        "html_id = \"\"\"\n",
        "<p style=\"color: black; font-size: 16px; font-weight: bold;\">\n",
        "    Checking _id mapping...<br>\n",
        "    Total unique IDs: <span style=\"color: green;\">{unique}</span>, Duplicates: <span style=\"color: green;\">{dups}</span>\n",
        "</p>\n",
        "\"\"\"\n",
        "display(HTML(html_id.format(unique=len(set(sample_ids)), dups=duplicated_ids)))\n",
        "if duplicated_ids > 0:\n",
        "    display(HTML(\"<p style=\\\"color: red; font-size: 16px; font-weight: bold;\\\">Warning: Duplicate _id values found!</p>\"))\n",
        "\n",
        "display(HTML(\"<p style=\\\"color: darkblue; font-size: 16px; font-weight: bold;\\\">Pre-clustering checks completed. Data is ready for clustering!</p>\"))\n",
        "\n",
        "                                                                                  # Prepare clustering results DataFrame\n",
        "clustering_results = original_data.copy()                                         # Create a copy to store clustering results\n",
        "for i in range(1, 6):                                                             # Add placeholder columns for clustering results for each set\n",
        "    for algo in ['KMeans','DBSCAN']:\n",
        "        clustering_results[f'{algo}{i}_Cluster'] = -1\n",
        "        clustering_results[f'{algo}{i}_Silhouette_Score'] = np.nan\n",
        "        clustering_results[f'{algo}{i}_Davies_Bouldin_Index'] = np.nan\n",
        "        if algo == \"KMeans\":\n",
        "            clustering_results[f'{algo}{i}_Calinski_Harabasz_Score'] = np.nan\n",
        "        clustering_results[f'{algo}{i}_Prediction_Accuracy'] = np.nan\n",
        "\n",
        "                                                                                  # Clustering for each feature set with debugging outputs\n",
        "debug_cluster = \"<p style=\\\"color: darkblue; font-size: 18px; font-weight: bold;\\\">Clustering Debug Info:<br>\"\n",
        "for i, features in enumerate(feature_sets, start=1):                              # Perform clustering on each feature set\n",
        "    valid_features = [f for f in features if f in sample_data.columns]\n",
        "    data_for_clustering = sample_data[valid_features].copy()\n",
        "    sample_ids = sample_data['_id'].values                                        # Store _id for mapping back\n",
        "    numerical_cols = data_for_clustering.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    scaler = StandardScaler()\n",
        "    data_scaled = pd.DataFrame(scaler.fit_transform(data_for_clustering[numerical_cols]), columns=numerical_cols)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=4, random_state=42)                                # KMeans Clustering\n",
        "    kmeans_labels = kmeans.fit_predict(data_scaled)\n",
        "    silhouette_score_kmeans = silhouette_score(data_scaled, kmeans_labels)\n",
        "    davies_bouldin_score_kmeans = davies_bouldin_score(data_scaled, kmeans_labels)\n",
        "    calinski_harabasz_score_kmeans = calinski_harabasz_score(data_scaled, kmeans_labels)\n",
        "    kmeans_accuracy = max(0, silhouette_score_kmeans) * 100\n",
        "\n",
        "    dbscan = DBSCAN(eps=0.5, min_samples=5)                                       # DBSCAN Clustering\n",
        "    dbscan_labels = dbscan.fit_predict(data_scaled)\n",
        "    silhouette_score_dbscan = -1 if len(set(dbscan_labels)) <= 1 else silhouette_score(data_scaled, dbscan_labels)\n",
        "    davies_bouldin_score_dbscan = -1 if len(set(dbscan_labels)) <= 1 else davies_bouldin_score(data_scaled, dbscan_labels)\n",
        "    dbscan_accuracy = max(0, silhouette_score_dbscan) * 100\n",
        "\n",
        "    debug_cluster += f\"Feature set <span style=\\\"color: darkblue;\\\">{set_names[i-1]}</span> - KMeans labels: <span style=\\\"color: green;\\\">{kmeans_labels[:10]}</span> ...<br>\"\n",
        "    debug_cluster += f\"Feature set <span style=\\\"color: darkblue;\\\">{set_names[i-1]}</span> - DBSCAN labels: <span style=\\\"color: green;\\\">{dbscan_labels[:10]}</span> ...<br>\"\n",
        "\n",
        "    for idx, original_idx in enumerate(sample_ids):                               # Map clustering results back to original data based on feature _id\n",
        "        clustering_results.loc[original_idx, f'KMeans{i}_Cluster'] = kmeans_labels[idx]\n",
        "        clustering_results.loc[original_idx, f'KMeans{i}_Silhouette_Score'] = silhouette_score_kmeans\n",
        "        clustering_results.loc[original_idx, f'KMeans{i}_Davies_Bouldin_Index'] = davies_bouldin_score_kmeans\n",
        "        clustering_results.loc[original_idx, f'KMeans{i}_Calinski_Harabasz_Score'] = calinski_harabasz_score_kmeans\n",
        "        clustering_results.loc[original_idx, f'KMeans{i}_Prediction_Accuracy'] = kmeans_accuracy\n",
        "        clustering_results.loc[original_idx, f'DBSCAN{i}_Cluster'] = dbscan_labels[idx]\n",
        "        clustering_results.loc[original_idx, f'DBSCAN{i}_Silhouette_Score'] = silhouette_score_dbscan\n",
        "        clustering_results.loc[original_idx, f'DBSCAN{i}_Davies_Bouldin_Index'] = davies_bouldin_score_dbscan\n",
        "        clustering_results.loc[original_idx, f'DBSCAN{i}_Prediction_Accuracy'] = dbscan_accuracy\n",
        "\n",
        "debug_cluster += \"</p>\"\n",
        "display(HTML(debug_cluster))\n",
        "\n",
        "                                                                                  # Debugging: Check for NaN values in clustering_results and print a few rows\n",
        "debug_nan = \"<ul style=\\\"color: darkblue; font-size: 18px; font-weight: bold;\\\"><li>Checking for NaN values in clustering_results: <span style=\\\"color: green;\\\">{nan_dict}</span></li><li>Preview of clustering_results (first 10 rows): <span style=\\\"color: green;\\\">{preview}</span></li></ul>\"\n",
        "nan_dict = clustering_results.isnull().sum().to_dict()\n",
        "preview = clustering_results.head(10).to_html(classes=\"table table-bordered\", index=False)\n",
        "display(HTML(debug_nan.format(nan_dict=nan_dict, preview=preview)))\n",
        "\n",
        "clustering_results.to_csv('Best_Clustering_Models.csv', index=False)                  # Save the clustering results to a CSV file\n",
        "display(HTML(\"\"\"\n",
        "    <p style=\"color: darkblue; font-size: 18px; font-weight: bold;\">\n",
        "         Clustering results saved as <span style=\"color: darkblue;\">'clustering_results.csv'</span>.\n",
        "    </p>\n",
        "\"\"\"))\n",
        "\n",
        "                                                                                  # Define the base columns (first file)\n",
        "base_columns = ['_id', 'EVENT_UNIQUE_ID', 'OCC_YEAR', 'OCC_MONTH', 'OCC_DAY', 'OCC_DOY', 'OCC_DOW', 'OCC_HOUR', 'DIVISION', 'LOCATION_TYPE', 'PREMISES_TYPE', 'HOOD_158', 'NEIGHBOURHOOD_158', 'LONG_WGS84', 'LAT_WGS84', 'OCC_DATETIME', 'reporting_delay_days', 'reporting_delay_hours', 'Location_Engineered', 'Hood_158_Encoded', 'Division_Encoded', 'Location_Engineered_Other', 'Location_Engineered_Public', 'Location_Engineered_Residential', 'OCC_Month_Encoded', 'OCC_DOW_Encoded']\n",
        "clustering_results_base = clustering_results[base_columns]                        # Create and save the base CSV file\n",
        "clustering_results_base.to_csv('Clustering_Base_Features.csv', index=False)\n",
        "\n",
        "                                                                                  # Define the clustering statistics columns (second file)\n",
        "stats_columns = ['_id',\n",
        "                 'KMeans1_Cluster', 'KMeans1_Silhouette_Score', 'KMeans1_Davies_Bouldin_Index', 'KMeans1_Calinski_Harabasz_Score', 'KMeans1_Prediction_Accuracy',\n",
        "                 'DBSCAN1_Cluster', 'DBSCAN1_Silhouette_Score', 'DBSCAN1_Davies_Bouldin_Index', 'DBSCAN1_Prediction_Accuracy',\n",
        "                 'KMeans2_Cluster', 'KMeans2_Silhouette_Score', 'KMeans2_Davies_Bouldin_Index', 'KMeans2_Calinski_Harabasz_Score', 'KMeans2_Prediction_Accuracy',\n",
        "                 'DBSCAN2_Cluster', 'DBSCAN2_Silhouette_Score', 'DBSCAN2_Davies_Bouldin_Index', 'DBSCAN2_Prediction_Accuracy',\n",
        "                 'KMeans3_Cluster', 'KMeans3_Silhouette_Score', 'KMeans3_Davies_Bouldin_Index', 'KMeans3_Calinski_Harabasz_Score', 'KMeans3_Prediction_Accuracy',\n",
        "                 'DBSCAN3_Cluster', 'DBSCAN3_Silhouette_Score', 'DBSCAN3_Davies_Bouldin_Index', 'DBSCAN3_Prediction_Accuracy',\n",
        "                 'KMeans4_Cluster', 'KMeans4_Silhouette_Score', 'KMeans4_Davies_Bouldin_Index', 'KMeans4_Calinski_Harabasz_Score', 'KMeans4_Prediction_Accuracy',\n",
        "                 'DBSCAN4_Cluster', 'DBSCAN4_Silhouette_Score', 'DBSCAN4_Davies_Bouldin_Index', 'DBSCAN4_Prediction_Accuracy',\n",
        "                 'KMeans5_Cluster', 'KMeans5_Silhouette_Score', 'KMeans5_Davies_Bouldin_Index', 'KMeans5_Calinski_Harabasz_Score', 'KMeans5_Prediction_Accuracy',\n",
        "                 'DBSCAN5_Cluster', 'DBSCAN5_Silhouette_Score', 'DBSCAN5_Davies_Bouldin_Index', 'DBSCAN5_Prediction_Accuracy']\n",
        "clustering_results_stats = clustering_results[stats_columns]                      # Create and save the clustering stats CSV file\n",
        "clustering_results_stats.to_csv('Clustering_Result_Stats.csv', index=False)\n",
        "files.download('Clustering_Result_Stats.csv')                                     # Download the clustering stats CSV file\n",
        "display(HTML(\"\"\"\n",
        "    <p style=\"color: darkblue; font-size: 18px; font-weight: bold;\">\n",
        "         Clustering Base Features saved as <span style=\"color: darkblue;\">'Clustering_Base_Features.csv'</span>.<br>\n",
        "         Clustering Statistics saved as <span style=\"color: darkblue;\">'Clustering_Result_Stats.csv'</span>.\n",
        "    </p>\n",
        "\"\"\"))\n",
        "files.download('Best_Clustering_Models.csv')                                      # Download the clustering base CSV file\n",
        "files.download('Clustering_Base_Features.csv')                                    # Download the clustering results CSV file\n",
        "files.download('Clustering_Result_Stats.csv')                                     # Download the feature combo results CSV file"
      ],
      "metadata": {
        "id": "yJTgOD5JhMvW"
      },
      "id": "yJTgOD5JhMvW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.0 Descriptive Statistics - Best Clustering Models - Approach_3**"
      ],
      "metadata": {
        "id": "iPXUrqyihdN6"
      },
      "id": "iPXUrqyihdN6"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/mohammadbadi/CrimeAnalytics_Clustering_Approach_3/refs/heads/main/Output_CSV/Best_Clustering_Results.csv\"\n",
        "url1 = \"https://raw.githubusercontent.com/mohammadbadi/CrimeAnalytics_Clustering_Approach_3/refs/heads/main/Output_CSV/Feature_Combo_Current_Results.csv\"\n",
        "clustering_results = pd.read_csv(url)\n",
        "\n",
        "# Load feature combinations\n",
        "feature_combos = pd.read_csv(url1)\n",
        "\n",
        "# Define the set names to match\n",
        "set_names = ['4_Set_165','4_Set_369', '4_Set_490', '4_Set_494', '4_Set_495']\n",
        "\n",
        "# Initialize an empty list to hold the feature sets\n",
        "feature_sets = []\n",
        "\n",
        "# Extract corresponding feature sets\n",
        "for set_name in set_names:\n",
        "    matched_features = feature_combos[feature_combos['Feature Set'] == set_name]['Feature_Names_String']\n",
        "    if not matched_features.empty:\n",
        "        features_list = ast.literal_eval(matched_features.values[0])  # Convert string to list\n",
        "        feature_sets.append(features_list)\n",
        "\n",
        "# Build a combined results list (one row per set)\n",
        "combined_results = []\n",
        "for i in range(1, 6):  # For each set (5 sets)\n",
        "    # KMeans metrics\n",
        "    silhouette_score_kmeans = clustering_results[f'KMeans{i}_Silhouette_Score'].iloc[0]\n",
        "    davies_bouldin_score_kmeans = clustering_results[f'KMeans{i}_Davies_Bouldin_Index'].iloc[0]\n",
        "    calinski_harabasz_score_kmeans = clustering_results[f'KMeans{i}_Calinski_Harabasz_Score'].iloc[0]\n",
        "    kmeans_accuracy = clustering_results[f'KMeans{i}_Prediction_Accuracy'].iloc[0]\n",
        "\n",
        "    # DBSCAN metrics\n",
        "    silhouette_score_dbscan = clustering_results[f'DBSCAN{i}_Silhouette_Score'].iloc[0]\n",
        "    davies_bouldin_score_dbscan = clustering_results[f'DBSCAN{i}_Davies_Bouldin_Index'].iloc[0]\n",
        "    dbscan_accuracy = clustering_results[f'DBSCAN{i}_Prediction_Accuracy'].iloc[0]\n",
        "\n",
        "    # Format the prediction accuracies (bold, 2 decimals)\n",
        "    kmeans_accuracy_str = f\"<strong>{kmeans_accuracy:.2f}%</strong>\"\n",
        "    dbscan_accuracy_str = f\"<strong>{dbscan_accuracy:.2f}%</strong>\"\n",
        "\n",
        "    # Create a combined record for this set\n",
        "    combined_results.append({\n",
        "         \"Set\": f\"Set {i}\",\n",
        "         \"Features\": ', '.join(feature_sets[i - 1]),\n",
        "         \"KMeans Silhouette Score\": f\"{silhouette_score_kmeans:.3f}\",\n",
        "         \"KMeans Davies-Bouldin Index\": f\"{davies_bouldin_score_kmeans:.3f}\",\n",
        "         \"KMeans Calinski-Harabasz Score\": f\"{calinski_harabasz_score_kmeans:.0f}\",\n",
        "         \"KMeans Prediction Accuracy\": kmeans_accuracy_str,\n",
        "         \"DBSCAN Silhouette Score\": f\"{silhouette_score_dbscan:.3f}\",\n",
        "         \"DBSCAN Davies-Bouldin Index\": f\"{davies_bouldin_score_dbscan:.3f}\",\n",
        "         \"DBSCAN Prediction Accuracy\": dbscan_accuracy_str,\n",
        "         \"DBSCAN Accuracy Float\": dbscan_accuracy  # for sorting purposes\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the combined results\n",
        "df_combined = pd.DataFrame(combined_results)\n",
        "\n",
        "# Sort the DataFrame by DBSCAN Accuracy (as a float) in descending order\n",
        "df_sorted = df_combined.sort_values(by=\"DBSCAN Accuracy Float\", ascending=False)\n",
        "\n",
        "# Build the HTML table using the sorted DataFrame\n",
        "html_table = \"\"\"\n",
        "<table style='border-collapse: collapse; width: 100%; font-size: 18px;'>\n",
        "  <thead style='background-color: #4CAF50; color: white;'>\n",
        "    <tr>\n",
        "      <th colspan=\"9\" style=\"text-align: center; font-size: 24px; background-color: #2f4f4f; color: white;\">\n",
        "        <strong>Clustering Summary Table</strong>\n",
        "      </th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>Set</th>\n",
        "      <th>Features</th>\n",
        "      <th>KMeans Silhouette Score</th>\n",
        "      <th>Davies-Bouldin Index</th>\n",
        "      <th>Calinski-Harabasz Score</th>\n",
        "      <th>KMeans Prediction Accuracy (%)</th>\n",
        "      <th>DBSCAN Silhouette Score</th>\n",
        "      <th>Davies-Bouldin Index</th>\n",
        "      <th>DBSCAN Prediction Accuracy (%)</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "\"\"\"\n",
        "\n",
        "# Loop through the sorted rows to build the table rows\n",
        "for idx, row in df_sorted.iterrows():\n",
        "    html_table += f\"\"\"\n",
        "    <tr style='border: 1px solid #dddddd;'>\n",
        "      <td style='border: 1px solid #dddddd; padding: 8px;'>{row['Set']}</td>\n",
        "      <td style='border: 1px solid #dddddd; padding: 8px;'>{row['Features']}</td>\n",
        "      <td style='border: 1px solid #dddddd; padding: 8px;'>{row['KMeans Silhouette Score']}</td>\n",
        "      <td style='border: 1px solid #dddddd; padding: 8px;'>{row['KMeans Davies-Bouldin Index']}</td>\n",
        "      <td style='border: 1px solid #dddddd; padding: 8px;'>{row['KMeans Calinski-Harabasz Score']}</td>\n",
        "      <td style='border: 1px solid #dddddd; padding: 8px;'>{row['KMeans Prediction Accuracy']}</td>\n",
        "      <td style='border: 1px solid #dddddd; padding: 8px;'>{row['DBSCAN Silhouette Score']}</td>\n",
        "      <td style='border: 1px solid #dddddd; padding: 8px;'>{row['DBSCAN Davies-Bouldin Index']}</td>\n",
        "      <td style='border: 1px solid #dddddd; padding: 8px;'>{row['DBSCAN Prediction Accuracy']}</td>\n",
        "    </tr>\n",
        "    \"\"\"\n",
        "html_table += \"</tbody></table>\"\n",
        "\n",
        "# Save the HTML table to a file\n",
        "with open(\"Best_Clusters_Summary_Results.html\", \"w\") as f:\n",
        "    f.write(html_table)\n",
        "    files.download(\"Best_Clusters_Summary_Results.html\")\n",
        "\n",
        "# Display the HTML table in Google Colab\n",
        "display(HTML(html_table))\n",
        "\n",
        "# Display formatted message for saved file\n",
        "display(HTML(\"\"\"\n",
        "    <p style=\"color: darkblue; font-size: 18px; font-weight: bold;\">\n",
        "        Clustering Model summary results saved  as <span style=\"color: green;\">Best_Clusters_Summary_Results.html</span>.\n",
        "    </p>\n",
        "\"\"\"))\n"
      ],
      "metadata": {
        "id": "4nOFbNIBhf3V"
      },
      "id": "4nOFbNIBhf3V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code Sections/6.1 3D Visualizations of Top 5 Clusters - Approach 3.ipynb**"
      ],
      "metadata": {
        "id": "l-n3_9RchhZt"
      },
      "id": "l-n3_9RchhZt"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaleido  # Uncomment if kaleido is not installed\n"
      ],
      "metadata": {
        "id": "Ly7azD_jhk4y"
      },
      "id": "Ly7azD_jhk4y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from IPython.display import display, HTML\n",
        "import ast, html, base64, os, copy\n",
        "\n",
        "#!pip install -q kaleido  # Uncomment if kaleido is not installed\n",
        "\n",
        "# Global variable to accumulate interactive HTML (if needed)\n",
        "all_html = \"\"\n",
        "\n",
        "# Display function for interactive models (800x600px container)\n",
        "def display_interactive_table(title, fig):\n",
        "    global all_html\n",
        "    fig.update_layout(autosize=True, height=580)  # Plot area height 580px\n",
        "    fig.update_traces(marker_line_width=0)\n",
        "    # Generate interactive HTML with modebar visible\n",
        "    fig_html = fig.to_html(full_html=False, include_plotlyjs='cdn', config={\"displayModeBar\": True})\n",
        "    fig_html_escaped = html.escape(fig_html)\n",
        "    iframe_html = f\"\"\"<iframe srcdoc=\"{fig_html_escaped}\" style=\"position:relative; width:800px; height:610px; border:none;\"></iframe>\"\"\"\n",
        "    html_table = f\"\"\"\n",
        "      <table style=\"border-collapse: collapse; width:800px; margin:auto; border:2px solid black;\">\n",
        "        <tbody>\n",
        "          <tr>\n",
        "              <td style=\"padding:0; margin:0;\">{iframe_html}</td>\n",
        "          </tr>\n",
        "        </tbody>\n",
        "      </table>\n",
        "      <br>\n",
        "      \"\"\"\n",
        "\n",
        "    all_html += html_table\n",
        "    display(HTML(html_table))\n",
        "\n",
        "# Functions to save and display static images\n",
        "def file_to_base64(filepath):\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        data = f.read()\n",
        "    return base64.b64encode(data).decode('utf-8')\n",
        "\n",
        "def display_static_image(filepath):\n",
        "    img_base64 = file_to_base64(filepath)\n",
        "    html_table = f\"\"\"\n",
        "    <table style=\"border-collapse: collapse; width:800px; margin:auto;\">\n",
        "      <tr>\n",
        "         <td style=\"border: 1px solid #dddddd; text-align:center;\">\n",
        "            <img src=\"data:image/png;base64,{img_base64}\" style=\"width:100%; border:1px solid #dddddd;\" />\n",
        "         </td>\n",
        "      </tr>\n",
        "    </table>\n",
        "    <br>\n",
        "    \"\"\"\n",
        "    display(HTML(html_table))\n",
        "\n",
        "# Load datasets\n",
        "url = \"https://raw.githubusercontent.com/mohammadbadi/CrimeAnalytics_Clustering_Approach_3/refs/heads/main/Output_CSV/Clustering_Base_Features.csv\"\n",
        "url1 = \"https://raw.githubusercontent.com/mohammadbadi/CrimeAnalytics_Clustering_Approach_3/refs/heads/main/Output_CSV/Clustering_Result_Stats.csv\"\n",
        "url2 = \"https://raw.githubusercontent.com/mohammadbadi/CrimeAnalytics_Clustering_Approach_3/refs/heads/main/Output_CSV/Feature_Combo_Current_Results.csv\"\n",
        "features_df = pd.read_csv(url)\n",
        "clustering_stats_df = pd.read_csv(url1)\n",
        "clustering_results = pd.merge(features_df, clustering_stats_df, on=\"_id\", how=\"left\")\n",
        "feature_combos = pd.read_csv(url2)\n",
        "\n",
        "# Extract feature sets\n",
        "set_names = ['4_Set_165', '4_Set_369', '4_Set_490', '4_Set_494', '4_Set_495']\n",
        "feature_sets = []\n",
        "for set_name in set_names:\n",
        "    matched_features = feature_combos[feature_combos['Feature Set'] == set_name]['Feature_Names_String']\n",
        "    if not matched_features.empty:\n",
        "        features_list = ast.literal_eval(matched_features.values[0])\n",
        "        feature_sets.append(features_list)\n",
        "\n",
        "color_sequence = px.colors.qualitative.Plotly\n",
        "static_outputs = []  # to store PNG filenames\n",
        "\n",
        "# Process clustering sets (KMeans and DBSCAN)\n",
        "for i in range(1, 6):\n",
        "    kmeans_cluster_col = f'KMeans{i}_Cluster'\n",
        "    dbscan_cluster_col = f'DBSCAN{i}_Cluster'\n",
        "    features_used = feature_sets[i - 1]\n",
        "\n",
        "    valid_kmeans_data = clustering_results[\n",
        "        clustering_results[kmeans_cluster_col].notna() &\n",
        "        (clustering_results[kmeans_cluster_col] != -1) &\n",
        "        clustering_results[features_used[0]].notna() &\n",
        "        clustering_results[features_used[1]].notna() &\n",
        "        clustering_results[features_used[2]].notna()\n",
        "    ]\n",
        "    valid_dbscan_data = clustering_results[\n",
        "        clustering_results[dbscan_cluster_col].notna() &\n",
        "        (clustering_results[dbscan_cluster_col] != -1) &\n",
        "        clustering_results[features_used[0]].notna() &\n",
        "        clustering_results[features_used[1]].notna() &\n",
        "        clustering_results[features_used[2]].notna()\n",
        "    ]\n",
        "\n",
        "    # Process KMeans data\n",
        "    if not valid_kmeans_data.empty:\n",
        "        cluster_sizes = valid_kmeans_data.groupby(kmeans_cluster_col).size().rename('cluster_size')\n",
        "        valid_kmeans_data = valid_kmeans_data.merge(cluster_sizes, left_on=kmeans_cluster_col, right_index=True)\n",
        "        fig_kmeans = px.scatter_3d(\n",
        "            valid_kmeans_data,\n",
        "            x=features_used[0],\n",
        "            y=features_used[1],\n",
        "            z=features_used[2],\n",
        "            color=valid_kmeans_data[kmeans_cluster_col].astype(str),\n",
        "            size='cluster_size',\n",
        "            size_max=50,\n",
        "            color_discrete_sequence=color_sequence\n",
        "        )\n",
        "        fig_kmeans.update_traces(marker_line_width=0)\n",
        "\n",
        "        # Update layout with smaller margins and horizontal legend\n",
        "        fig_kmeans.update_layout(\n",
        "            margin=dict(t=20, b=1, l=30, r=1),  # Reduced top margin\n",
        "            scene=dict(\n",
        "                xaxis_title=features_used[0],\n",
        "                yaxis_title=features_used[1],\n",
        "                zaxis_title=features_used[2],\n",
        "                domain=dict(x=[0.2, 1], y=[0, 1])\n",
        "            ),\n",
        "            legend=dict(\n",
        "                orientation=\"v\",    # Horizontal legend\n",
        "                yanchor=\"top\",      # Anchor position\n",
        "                y=0.68,             # Position at top\n",
        "                xanchor=\"left\",     # Left align horizontally\n",
        "                x=0,                # Left position\n",
        "                itemwidth=30,       # Width of each legend item\n",
        "                itemsizing=\"constant\", # Fixed size for items\n",
        "                borderwidth=0,      # Add a border\n",
        "                bordercolor=\"gray\", # Border color\n",
        "                tracegroupgap=7     # Gap between legend groups\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Add plot title as an annotation in the figure\n",
        "        fig_kmeans.add_annotation(\n",
        "            x=0.1, y=1.018, xref=\"paper\", yref=\"paper\",\n",
        "            text=f\"<b>{'KMeans Clustering Set ' + str(i)}</b>\",\n",
        "            showarrow=False, align=\"center\",\n",
        "            font=dict(size=22),\n",
        "            bgcolor=\"rgba(255,255,255,0.8)\",\n",
        "            bordercolor=\"gray\",\n",
        "            borderwidth=0,\n",
        "            borderpad=2\n",
        "        )\n",
        "        # Add feature names as an annotation near the top, left-aligned\n",
        "        wrapped_features = \"<b>Features:</b> \" + \",<br>\".join(features_used)\n",
        "        fig_kmeans.add_annotation(\n",
        "            x=-0.04, y=0.9, xref=\"paper\", yref=\"paper\",\n",
        "            text=wrapped_features,\n",
        "            showarrow=False, align=\"left\",\n",
        "            font=dict(size=15),\n",
        "            bgcolor=\"rgba(255,255,255,0.8)\",\n",
        "            bordercolor=\"gray\",\n",
        "            borderwidth=0,\n",
        "            borderpad=0\n",
        "        )\n",
        "        # Add clustering statistics as an annotation below the features annotation\n",
        "        kmeans_sil = clustering_results[f'KMeans{i}_Silhouette_Score'].iloc[0]\n",
        "        kmeans_db  = clustering_results[f'KMeans{i}_Davies_Bouldin_Index'].iloc[0]\n",
        "        kmeans_ch  = clustering_results[f'KMeans{i}_Calinski_Harabasz_Score'].iloc[0]\n",
        "        kmeans_acc = clustering_results[f'KMeans{i}_Prediction_Accuracy'].iloc[0]\n",
        "        kmeans_stats = (\n",
        "            f\"Silhouette: {kmeans_sil:.3f}<br>\"\n",
        "            f\"DB: {kmeans_db:.3f}<br>\"\n",
        "            f\"CH: {kmeans_ch:.0f}<br>\"\n",
        "            f\"Accuracy: {kmeans_acc:.2f}%\"\n",
        "          )\n",
        "        fig_kmeans.add_annotation(\n",
        "        x=-0.04, y=0, xref=\"paper\", yref=\"paper\",\n",
        "        text=f\"<b>{kmeans_stats}</b>\",\n",
        "        showarrow=False, align=\"left\",\n",
        "        font=dict(size=16),\n",
        "        bgcolor=\"rgba(255,255,255,0.8)\",\n",
        "        bordercolor=\"gray\",\n",
        "        borderwidth=0,\n",
        "        borderpad=0\n",
        "        )\n",
        "\n",
        "        # Display interactive figure\n",
        "        display_interactive_table(f\"KMeans Clustering Set {i}\", fig_kmeans)\n",
        "\n",
        "        # For static PNG - create a copy with different annotations\n",
        "        fig_kmeans_static = copy.deepcopy(fig_kmeans)\n",
        "\n",
        "        # Save static PNG for now (we'll improve it later)\n",
        "        filename = f\"kmeans_cluster_set_{i}.png\"\n",
        "        fig_kmeans_static.write_image(filename, width=800, height=600)\n",
        "        static_outputs.append(filename)\n",
        "\n",
        "    # Process DBSCAN data with the same improvements\n",
        "    if not valid_dbscan_data.empty:\n",
        "        cluster_sizes = valid_dbscan_data.groupby(dbscan_cluster_col).size().rename('cluster_size')\n",
        "        valid_dbscan_data = valid_dbscan_data.merge(cluster_sizes, left_on=dbscan_cluster_col, right_index=True)\n",
        "        fig_dbscan = px.scatter_3d(\n",
        "            valid_dbscan_data,\n",
        "            x=features_used[0],\n",
        "            y=features_used[1],\n",
        "            z=features_used[2],\n",
        "            color=valid_dbscan_data[dbscan_cluster_col].astype(str),\n",
        "            size='cluster_size',\n",
        "            size_max=50,\n",
        "            color_discrete_sequence=color_sequence\n",
        "        )\n",
        "        fig_dbscan.update_traces(marker_line_width=0)\n",
        "\n",
        "        # Update layout with smaller margins and horizontal legend\n",
        "        # For DBSCAN plots - update this section\n",
        "        fig_dbscan.update_layout(\n",
        "            margin=dict(t=20, b=1, l=30, r=1),  # Reduced top margin\n",
        "            scene=dict(\n",
        "                xaxis_title=features_used[0],\n",
        "                yaxis_title=features_used[1],\n",
        "                zaxis_title=features_used[2],\n",
        "                domain=dict(x=[0.2, 1], y=[0, 1])\n",
        "            ),\n",
        "            legend=dict(\n",
        "                orientation=\"v\",    # Horizontal legend\n",
        "                yanchor=\"top\",      # Anchor position\n",
        "                y=0.68,             # Position at top\n",
        "                xanchor=\"left\",     # Left align horizontally\n",
        "                x=0,                # Left position\n",
        "                itemwidth=30,       # Width of each legend item\n",
        "                itemsizing=\"constant\", # Fixed size for items\n",
        "                borderwidth=0,      # Add a border\n",
        "                bordercolor=\"gray\", # Border color\n",
        "                tracegroupgap=7     # Gap between legend groups\n",
        "            )\n",
        "        )\n",
        "        # Add plot title as an annotation in the figure\n",
        "        fig_dbscan.add_annotation(\n",
        "            x=0.1, y=1.018, xref=\"paper\", yref=\"paper\",\n",
        "            text=f\"<b>{'DBSCAN Clustering Set ' + str(i)}</b>\",\n",
        "            showarrow=False, align=\"center\",\n",
        "            font=dict(size=22),\n",
        "            bgcolor=\"rgba(255,255,255,0.8)\",\n",
        "            bordercolor=\"gray\",\n",
        "            borderwidth=0,\n",
        "            borderpad=2\n",
        "        )\n",
        "        # Add feature names as an annotation near the top, left-aligned\n",
        "        wrapped_features = \"<b>Features:</b> \" + \",<br>\".join(features_used)\n",
        "        fig_dbscan.add_annotation(\n",
        "            x=-0.04, y=0.90, xref=\"paper\", yref=\"paper\",\n",
        "            text=wrapped_features,\n",
        "            showarrow=False, align=\"left\",\n",
        "            font=dict(size=15),\n",
        "            bgcolor=\"rgba(255,255,255,0.8)\",\n",
        "            bordercolor=\"gray\",\n",
        "            borderwidth=0,\n",
        "            borderpad=0\n",
        "        )\n",
        "        # Add clustering statistics as an annotation below the features annotation\n",
        "        dbscan_sil = clustering_results[f'DBSCAN{i}_Silhouette_Score'].iloc[0]\n",
        "        dbscan_db  = clustering_results[f'DBSCAN{i}_Davies_Bouldin_Index'].iloc[0]\n",
        "        dbscan_acc = clustering_results[f'DBSCAN{i}_Prediction_Accuracy'].iloc[0]\n",
        "        dbscan_stats = (\n",
        "                  f\"Silhouette: {dbscan_sil:.3f}<br>\"\n",
        "                  f\"DB: {dbscan_db:.3f}<br>\"\n",
        "                  f\"Accuracy: {dbscan_acc:.2f}%\"\n",
        "                )\n",
        "        fig_dbscan.add_annotation(\n",
        "        x=-0.04, y=0, xref=\"paper\", yref=\"paper\",\n",
        "        text=f\"<b>{dbscan_stats}</b>\",\n",
        "        showarrow=False, align=\"left\",\n",
        "        font=dict(size=16),\n",
        "        bgcolor=\"rgba(255,255,255,0.8)\",\n",
        "        bordercolor=\"gray\",\n",
        "        borderwidth=0,\n",
        "        borderpad=0\n",
        "        )\n",
        "\n",
        "        # Display interactive figure\n",
        "        display_interactive_table(f\"DBSCAN Clustering Set {i}\", fig_dbscan)\n",
        "\n",
        "        # For static PNG - create a copy with different annotations\n",
        "        fig_dbscan_static = copy.deepcopy(fig_dbscan)\n",
        "\n",
        "        # Save static PNG for now (we'll improve it later)\n",
        "        filename = f\"dbscan_cluster_set_{i}.png\"\n",
        "        fig_dbscan_static.write_image(filename, width=800, height=600)\n",
        "        static_outputs.append(filename)\n",
        "\n",
        "# After all interactive figures are displayed, show all static PNG images.\n",
        "for filepath in static_outputs:\n",
        "    if os.path.exists(filepath):\n",
        "        display_static_image(filepath)\n",
        "    else:\n",
        "        display(HTML(f\"<p style='text-align:center; color:red;'>Error: {filepath} not found.</p>\"))\n",
        "\n",
        "# Optionally, save all interactive HTML to a file.\n",
        "with open(\"interactive_visuals.html\", \"w\") as f:\n",
        "    f.write(all_html)\n",
        "\n",
        "display(HTML(\"<h3 style='text-align:center;'>Interactive HTML and static PNG images have been saved.</h3>\"))"
      ],
      "metadata": {
        "id": "CAPTnk3rhlUp"
      },
      "id": "CAPTnk3rhlUp",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 6888578,
          "sourceId": 11056653,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 31.616676,
      "end_time": "2025-03-17T09:44:10.110823",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-03-17T09:43:38.494147",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}